{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21e83b0a",
   "metadata": {},
   "source": [
    "# Tutorial: Building a GRU Language Model with PyTorch\n",
    " \n",
    "**Target Audience:** MSc Computer Science Students  \n",
    "**Topic:** Natural Language Processing, Recurrent Neural Networks  \n",
    " \n",
    "## 1. Introduction\n",
    " \n",
    "In this tutorial, we will build a generative Language Model (LM) using a Gated Recurrent Unit (GRU). The goal of a language model is to predict the next token in a sequence given the previous context. Formally, given a sequence of tokens $x_1, x_2, ..., x_t$, the model attempts to learn the probability distribution:\n",
    " \n",
    "$P(x_{t+1} | x_1, ..., x_t) $\n",
    " \n",
    "We will use a **Character-Level** model for this demonstration. This means the model processes text character-by-character (including spaces and punctuation). While less semantic than word-level models, character-level models have a smaller vocabulary and are excellent for demonstrating the mechanics of RNNs without massive datasets.\n",
    "\n",
    "## 2. Theoretical Background: The GRU\n",
    "\n",
    "Standard Recurrent Neural Networks (RNNs) suffer from the **vanishing gradient problem**. During backpropagation through time (BPTT), gradients can shrink exponentially as they propagate backward, making it difficult for the model to learn long-range dependencies.\n",
    " \n",
    "The *Gated Recurrent Unit (GRU)*, introduced by Cho et al. (2014), solves this using gating mechanisms that regulate the flow of information. It is mathematically similar to the LSTM but more computationally efficient.\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "At time step $t$, given input $x_t$ and previous hidden state $h_{t-1}$, the GRU computes:\n",
    "\n",
    "### 1.  *Reset Gate ($r_t$):* Determines how much of the past information to forget.\n",
    "### $r_t = \\sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{t-1} + b_{hr})$\n",
    "\n",
    "### 2.  *Update Gate ($z_t$):* Determines how much of the past state to carry over to the new state.\n",
    "### $z_t = \\sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{t-1} + b_{hz})$\n",
    "\n",
    "### 3.  *Candidate Hidden State ($\\tilde{n}_t$):* Computes new candidate information.\n",
    "### $\\tilde{n}_t = \\tanh(W_{in} x_t + b_{in} + r_t \\odot (W_{hn} h_{t-1} + b_{hn})) $\n",
    "\n",
    "### 4.  *Final Hidden State ($h_t$):* Linear interpolation between the previous state and the candidate state.\n",
    "### $ h_t = (1 - z_t) \\odot \\tilde{n}_t + z_t \\odot h_{t-1} $\n",
    "\n",
    "Where $\\sigma$ is the sigmoid function, $\\odot$ is the Hadamard (element-wise) product, and $W$ and $b$ are learnable weights and biases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1645eb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e93af3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# --- CRITICAL IMPORTS FOR TPU/XLA ---\n",
    "# Note: torch_xla is only needed for Google Cloud TPU environments\n",
    "# This notebook will work fine without it on CPU/CUDA/MPS\n",
    "XLA_AVAILABLE = False\n",
    "xm = None\n",
    "\n",
    "# Uncomment the following lines if running on TPU:\n",
    "# try:\n",
    "#     import torch_xla.core.xla_model as xm\n",
    "#     XLA_AVAILABLE = True\n",
    "# except ImportError:\n",
    "#     XLA_AVAILABLE = False\n",
    "#     print(\"WARNING: torch_xla not found. Running on CPU/CUDA fallback.\")\n",
    "# --- END XLA IMPORTS ---\n",
    "\n",
    "# Set device for PyTorch operations\n",
    "if XLA_AVAILABLE:\n",
    "    # Use xm.xla_device() to get the primary TPU core device\n",
    "    DEVICE = xm.xla_device()\n",
    "    N_DEVICES = 1 # Force single device count\n",
    "    print(f\"Using Single XLA Device: {DEVICE}\")\n",
    "elif torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    DEVICE = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device('mps')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "print(f'Using device: {DEVICE}')\n",
    "\n",
    "\n",
    "# Uncomment the following lines if running on Google Colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440b7d1a",
   "metadata": {},
   "source": [
    "## 3. The Dataset\n",
    "\n",
    "### For this tutorial, we will use a small dataset of Aesop's Fables. In a real-world scenario, you might use Project Gutenberg texts or the WikiText dataset.\n",
    "### We simulate loading a file by defining a raw string below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1b22504",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_TEXT = \"\"\"\n",
    "Title: The Wolf and the Lamb.\n",
    "Wolf, meeting with a Lamb astray from the fold, resolved not to lay violent hands on him, but to find some plea to justify to the Lamb the Wolf's right to eat him. \n",
    "He thus addressed him: \"Sirrah, last year you grossly insulted me.\" \n",
    "\"Indeed,\" bleated the Lamb in a mournful tone of voice, \"I was not then born.\" \n",
    "Then said the Wolf, \"You feed in my pasture.\" \n",
    "\"No, good sir,\" replied the Lamb, \"I have not yet tasted grass.\" \n",
    "Again said the Wolf, \"You drink of my well.\" \n",
    "\"No,\" exclaimed the Lamb, \"I never yet drank water, for as yet my mother's milk is both food and drink to me.\n",
    "\" Upon which the Wolf seized him and ate him up, saying, \"Well! I won't remain supperless, even though you refute every one of my imputations.\n",
    "\" The tyrant will always find a pretext for his tyranny.\n",
    "\n",
    "Title: The Bat and the Weasels.\n",
    "A Bat who fell upon the ground and was caught by a Weasel pleaded to be spared his life. \n",
    "The Weasel refused, saying that he was by nature the enemy of all birds. \n",
    "The Bat assured him that he was not a bird, but a mouse, and thus was set free. \n",
    "Shortly afterwards the Bat again fell to the ground and was caught by another Weasel, whom he likewise entreated not to eat him. \n",
    "The Weasel said that he had a special hostility to mice. \n",
    "The Bat assured him that he was not a mouse, but a bird, and thus escaped. \n",
    "It is wise to turn circumstances to good account.\n",
    "\n",
    "Title: The Ass and the Grasshopper.\n",
    "An Ass having heard some Grasshoppers chirping, was highly enchanted; and, desiring to possess the like charms of melody, demanded what sort of food they lived on to give them such beautiful voices. \n",
    "They replied, \"The dew.\" The Ass resolved that he would live only upon dew, and in a short time died of hunger.\n",
    "\n",
    "Title: The Lion and the Mouse.\n",
    "A Lion was awakened from sleep by a Mouse running over his face. \n",
    "Rising up angrily, he caught him and was about to kill him, when the Mouse piteously entreated, saying: \"If you would only spare my life, I would be sure to repay your kindness.\" \n",
    "The Lion laughed and let him go. \n",
    "It happened shortly after this that the Lion was caught by some hunters, who bound him by strong ropes to the ground. \n",
    "The Mouse, recognizing his roar, came and gnawed the rope with his teeth, and set him free, exclaiming: \"You ridiculed the idea of my ever being able to help you, not expecting to receive from me any repayment of your favor; now you know that it is possible for even a Mouse to con benefits on a Lion.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41092587",
   "metadata": {},
   "source": [
    "## Preprocessing: Clean slightly and analyze stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84e2f41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 2487 characters\n"
     ]
    }
   ],
   "source": [
    "text = RAW_TEXT.strip()\n",
    "print(f\"Dataset length: {len(text)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3657efbb",
   "metadata": {},
   "source": [
    "## 4. Preprocessing\n",
    " \n",
    "### Neural Networks operate on numbers (tensors), not strings. We need to pipeline our data:\n",
    "#### 1.  **Vocabulary Construction:** Find the set of unique characters.\n",
    "#### 2.  **Indexing:** Create mappings `char -> int` and `int -> char`.\n",
    "#### 3.  **Sequence Generation:** Create sliding windows of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52ab4f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique characters: 48\n",
      "Vocabulary: \n",
      " !\"',.:;ABGHILMNRSTUWYabcdefghijklmnoprstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# 1. Vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"Unique characters: {vocab_size}\")\n",
    "print(f\"Vocabulary: {''.join(chars)}\")\n",
    "\n",
    "# 2. Mappings\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# 3. Encode the entire text\n",
    "encoded_text = [char_to_idx[ch] for ch in text]\n",
    "encoded_tensor = torch.tensor(encoded_text, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64795fa3",
   "metadata": {},
   "source": [
    "## Custom Dataset Class\n",
    " \n",
    "#### We will create sequences of length `seq_len`. \n",
    "* **Input ($x$):** Characters at indices $[i, i+seq\\_len-1]$\n",
    "* **Target ($y$):** Characters at indices $[i+1, i+seq\\_len]$\n",
    " \n",
    "#### For example, if text is \"HELLO\" and `seq_len`=3:\n",
    "* Input: \"HEL\"\n",
    "* Target: \"ELL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68ddc744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch shape: torch.Size([32, 100])\n",
      "Target batch shape: torch.Size([32, 100])\n"
     ]
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_tensor, seq_len):\n",
    "        self.text_tensor = text_tensor\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        # We can fit len(text) - seq_len sequences\n",
    "        return len(self.text_tensor) - self.seq_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Input sequence\n",
    "        input_seq = self.text_tensor[idx : idx + self.seq_len]\n",
    "        # Target sequence (shifted by 1)\n",
    "        target_seq = self.text_tensor[idx + 1 : idx + self.seq_len + 1]\n",
    "        \n",
    "        return input_seq, target_seq\n",
    "\n",
    "# Hyperparameters for Data\n",
    "SEQ_LEN = 100\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "dataset = TextDataset(encoded_tensor, SEQ_LEN)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "# Verify shapes\n",
    "x_batch, y_batch = next(iter(dataloader))\n",
    "print(f\"Input batch shape: {x_batch.shape}\") # [Batch, Seq_Len]\n",
    "print(f\"Target batch shape: {y_batch.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c1319f",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Model Architecture\n",
    "\n",
    "### We will define a `GRULanguageModel` class inheriting from `nn.Module`.\n",
    "\n",
    "### **Architecture:**\n",
    "### 1.  **Embedding Layer:** Converts integer indices to dense vectors.\n",
    "###     * Input: `(Batch, Seq_Len)`\n",
    "###     * Output: `(Batch, Seq_Len, Embedding_Dim)`\n",
    "### 2.  **GRU Layer:** Processes the sequence.\n",
    "###     * Input: `(Batch, Seq_Len, Embedding_Dim)`\n",
    "###     * Output: `(Batch, Seq_Len, Hidden_Dim)`\n",
    "### 3.  **Fully Connected (Linear) Layer:** Maps hidden state to vocabulary logits.\n",
    "###     * Input: `(Batch, Seq_Len, Hidden_Dim)`\n",
    "###     * Output: `(Batch, Seq_Len, Vocab_Size)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3932cc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRULanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1):\n",
    "        super(GRULanguageModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # 1. Embedding Layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # 2. GRU Layer\n",
    "        # batch_first=True expects input shape (batch, seq, feature)\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # 3. Output Layer\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        # x shape: (batch_size, seq_len)\n",
    "        \n",
    "        # Embed: (batch_size, seq_len, embedding_dim)\n",
    "        embeds = self.embedding(x)\n",
    "        \n",
    "        # GRU Forward\n",
    "        # out shape: (batch_size, seq_len, hidden_dim)\n",
    "        # hidden shape: (num_layers, batch_size, hidden_dim)\n",
    "        out, hidden = self.gru(embeds, hidden)\n",
    "        \n",
    "        # Flatten output for Linear layer\n",
    "        # reshape to (batch_size * seq_len, hidden_dim)\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # Project to vocabulary size\n",
    "        # out shape: (batch_size * seq_len, vocab_size)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Initialize hidden state with zeros\n",
    "        weight = next(self.parameters()).data\n",
    "        return weight.new(self.num_layers, batch_size, self.hidden_dim).zero_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f6d6e4",
   "metadata": {},
   "source": [
    "## 6. Training Loop\n",
    "\n",
    "### We use **CrossEntropyLoss** which combines `LogSoftmax` and `NLLLoss`.\n",
    "### Note on dimensions: PyTorch's `CrossEntropyLoss` expects:\n",
    "### * Input (Logits): $(N, C)$ where $C$ is class count (vocab size).\n",
    "### * Target: $(N)$ where values are indices $[0, C-1]$.\n",
    "### This is why we flattened the output in the `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eff7520e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRULanguageModel(\n",
      "  (embedding): Embedding(48, 128)\n",
      "  (gru): GRU(128, 256, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=256, out_features=48, bias=True)\n",
      ")\n",
      "Bi-directional GRU Model Parameters: 709,680\n"
     ]
    }
   ],
   "source": [
    "# Model Hyperparameters\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 2\n",
    "LEARNING_RATE = 0.002\n",
    "EPOCHS = 50\n",
    "\n",
    "# Instantiate Model, Loss, and Optimizer\n",
    "model = GRULanguageModel(vocab_size, EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "print(model)\n",
    "\n",
    "# Calculate total trainable parameters\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Bi-directional GRU Model Parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48a93935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 50 epochs...\n",
      "Epoch 10/50 | Loss: 0.0883\n",
      "Epoch 20/50 | Loss: 0.0802\n",
      "Epoch 30/50 | Loss: 0.0761\n",
      "Epoch 40/50 | Loss: 0.0750\n",
      "Epoch 50/50 | Loss: 0.0736\n",
      "Training finished in 249.26s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Plotting Loss\\nplt.figure(figsize=(10,5))\\nplt.plot(loss_history)\\nplt.title(\"Training Loss\")\\nplt.xlabel(\"Epoch\")\\nplt.ylabel(\"Loss\")\\nplt.show()\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "loss_history = []\n",
    "model.train()\n",
    "\n",
    "print(f\"Starting training for {EPOCHS} epochs...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Initialize hidden state for the batch\n",
    "    # Note: In stateful RNN training, we might detach hidden states between batches \n",
    "    # to prevent backpropagating through the entire dataset history, \n",
    "    # but keep the values to maintain context. \n",
    "    # Here, for simplicity in a shuffled loader, we init hidden per batch.\n",
    "    h = model.init_hidden(BATCH_SIZE)\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        \n",
    "        # Detach hidden state to prevent backpropagating through history \n",
    "        # beyond the current batch (Truncated BPTT)\n",
    "        h = h.data \n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output, h = model(x, h)\n",
    "        \n",
    "        # Reshape target to align with output (batch_size * seq_len)\n",
    "        y = y.view(-1)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(output, y)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient Clipping (prevents exploding gradients, common in RNNs)\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        \n",
    "        if XLA_AVAILABLE:\n",
    "            # XLA specific optimization step\n",
    "            xm.optimizer_step(optimizer)\n",
    "            xm.mark_step() # Signal end of computation step to XLA\n",
    "        else:\n",
    "            optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    loss_history.append(avg_loss)\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(f\"Training finished in {time.time()-start_time:.2f}s\")\n",
    "\n",
    "# Save the model\n",
    "# Use xm.save if XLA is available, otherwise standard torch.save\n",
    "save_model = xm.save if XLA_AVAILABLE else torch.save\n",
    "save_model(model.state_dict(), \"gru_chara_lm.pth\")\n",
    "\n",
    "\"\"\"\n",
    "# Plotting Loss\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(loss_history)\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415baf40",
   "metadata": {},
   "source": [
    "## 7. Text Generation\n",
    "\n",
    "### To generate text, we:\n",
    "### 1.  Feed a \"seed\" string into the model to build up the hidden state.\n",
    "### 2.  Predict the next character probability distribution.\n",
    "### 3.  **Sample** from this distribution (don't just take the `argmax`, or the text will be repetitive).\n",
    "\n",
    "### Temperature Sampling\n",
    "### We use a hyperparameter $T$ (temperature) to control randomness.\n",
    "### $$ P_i = \\frac{\\exp(z_i / T)}{\\sum \\exp(z_j / T)} $$\n",
    "### **High T (>1.0):** Flattens distribution (more random/creative).\n",
    "### **Low T (<1.0):** Sharpens distribution (more confident/conservative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "deeec66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generated Text (T=0.8) ---\n",
      "The Lion feed him and ate him up, saying, \"Well! I won't remain supperless, even though you refute every one of my imputations.\n",
      "\" The tyrant will always find a pretext for his tyranny.\n",
      "\n",
      "Title: The Bat and the Weasels.\n",
      "A Bat who fell upon the ground and was caught by another Weasel, whom he likewise entreate\n",
      "\n",
      "--- Generated Text (T=0.2, Conservative) ---\n",
      "The Lion and the Mouse.\n",
      "A Lion was awakened from sleep by a Mouse running over his face. \n",
      "Rising up angrily, he caught him and was about to kill him, when the Mouse piteously entreated, saying: \"If you would only spare my life, I would be sure to repay your kindness.\" \n",
      "The Lion laughed and let him go. \n",
      "It h\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_str=\"The\", len_generated=200, temperature=0.8):\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize hidden state (batch size 1)\n",
    "    hidden = model.init_hidden(1)\n",
    "    \n",
    "    input_seq = torch.tensor([char_to_idx[ch] for ch in start_str]).unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    generated_text = start_str\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 1. Build up hidden state with start_str\n",
    "        # We run the whole seed sequence through. \n",
    "        # We only care about the hidden state output after the last char.\n",
    "        out, hidden = model(input_seq, hidden)\n",
    "        \n",
    "        # The input for the next step is the last character of the seed\n",
    "        last_char_idx = input_seq[:, -1]\n",
    "        \n",
    "        for i in range(len_generated):\n",
    "            # Forward pass with single character\n",
    "            # Reshape input to (1, 1) -> (Batch, Seq)\n",
    "            current_input = last_char_idx.unsqueeze(1)\n",
    "            \n",
    "            # Embed and GRU\n",
    "            # Note: We manually perform the forward logic here because our \n",
    "            # model.forward flattens output, which is slightly annoying for generation loop.\n",
    "            # Let's use the model components directly for clarity.\n",
    "            \n",
    "            emb = model.embedding(current_input)\n",
    "            out, hidden = model.gru(emb, hidden)\n",
    "            \n",
    "            # Output shape: (1, 1, hidden) -> (1, hidden)\n",
    "            out = out.squeeze(1)\n",
    "            logits = model.fc(out)\n",
    "            \n",
    "            # Apply temperature\n",
    "            probs = torch.softmax(logits / temperature, dim=1).cpu().numpy()[0]\n",
    "            \n",
    "            # Sample from distribution\n",
    "            next_char_idx = np.random.choice(vocab_size, p=probs)\n",
    "            \n",
    "            # Append result\n",
    "            generated_text += idx_to_char[next_char_idx]\n",
    "            \n",
    "            # Update input for next step\n",
    "            last_char_idx = torch.tensor([next_char_idx]).to(DEVICE)\n",
    "            \n",
    "    return generated_text\n",
    "\n",
    "# Model Hyperparameters\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 2\n",
    "\n",
    "# Instantiate and load the model\n",
    "model = GRULanguageModel(vocab_size, EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS).to(DEVICE)\n",
    "model.load_state_dict(torch.load(\"gru_chara_lm.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Demo 1: Moderate Temperature\n",
    "print(\"--- Generated Text (T=0.8) ---\")\n",
    "print(generate_text(model, start_str=\"The Lion\", len_generated=300, temperature=0.8))\n",
    "\n",
    "print(\"\\n--- Generated Text (T=0.2, Conservative) ---\")\n",
    "print(generate_text(model, start_str=\"The Lion\", len_generated=300, temperature=0.2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
