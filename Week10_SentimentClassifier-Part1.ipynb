{"cells":[{"cell_type":"markdown","id":"7e8d3e56","metadata":{"id":"7e8d3e56"},"source":["## Sentiment Classification Tutorial: Classical vs. Deep Learning Models (IMDB Reviews)\n","\n","This tutorial adapts the text classification framework to a binary sentiment analysis task using the IMDB Movie Reviews dataset. We will classify customer reviews into two categories: 'positive' or 'negative'.\n","\n","We will compare the performance of:\n","\n","* **Bag-of-Words (BoW) Classifier using TF-IDF features.**\n","\n","* **Gated Recurrent Unit (GRU) from PyTorch.**\n","\n","* **Bi-directional GRU (Bi-GRU) from PyTorch.**\n","\n","\n","#### Dataset: IMDB-Dataset.csv (The actual uploaded file is now used)\n","\n","#### Target: Binary Classification (2 classes: Positive, Negative)\n","\n","### In this Notebook, we will focus on Bag-of-Words Classifier."]},{"cell_type":"markdown","id":"7889ee41","metadata":{"id":"7889ee41"},"source":["### 1. **Setup and Data Loading**\n","\n","We import necessary libraries and load the IMDB dataset, converting the categorical sentiment labels into numerical format."]},{"cell_type":"code","execution_count":null,"id":"a7483d71","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a7483d71","executionInfo":{"status":"ok","timestamp":1764434549449,"user_tz":0,"elapsed":4348,"user":{"displayName":"Rahul Mourya","userId":"07975233105233353508"}},"outputId":"480d674f-b168-4362-c030-64a48f285529"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","\n","Training Samples: 40000\n","Testing Samples: 10000\n","Total Classes: 2\n","Classes: ['negative', 'positive']\n","--------------------------------------------------\n","Example Data Point (Class: positive):\n","I caught this little gem totally by accident back in 1980 or '81. I was at a revival theatre to see two old silly sci-fi movies. The theatre was packed full and (with no warning) they showed a bunch of sci-fi short spoofs (to get us in the mood). Most were somewhat amusing but THIS came on and, with...\n","\n"]}],"source":["# Core data science and NLP libraries\n","import numpy as np\n","import pandas as pd\n","import re\n","import os\n","\n","# Scikit-learn for classical ML\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, f1_score, classification_report\n","from sklearn.model_selection import train_test_split\n","from sklearn.pipeline import Pipeline\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","\n","# Set a random seed for reproducibility\n","SEED = 42\n","np.random.seed(SEED)\n","\n","# --- Load the IMDB Dataset ---\n","FILE_PATH = '/content/drive/MyDrive/Colab Notebooks/IMDB-Dataset.csv'\n","df = pd.read_csv(FILE_PATH)\n","\n","\n","# 1. Label Encoding: Convert 'positive' to 1 and 'negative' to 0\n","# The 'sentiment' column is the raw target string\n","df['target'] = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n","\n","X = df['review'].values\n","y = df['target'].values\n","\n","# 2. Split Data into Training and Testing Sets\n","# We use a standard 80/20 split for training and testing\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y,\n","    test_size=0.2,\n","    random_state=SEED,\n","    stratify=y # Ensure equal class distribution in both splits\n",")\n","\n","target_names = ['negative', 'positive'] # Map numerical targets back to names (0, 1)\n","\n","print(f\"\\nTraining Samples: {len(X_train)}\")\n","print(f\"Testing Samples: {len(X_test)}\")\n","print(f\"Total Classes: {len(target_names)}\")\n","print(f\"Classes: {target_names}\")\n","print(\"-\" * 50)\n","print(f\"Example Data Point (Class: {target_names[y_train[0]]}):\\n{X_train[0][:300]}...\\n\")\n"]},{"cell_type":"markdown","id":"5ee4ceb1","metadata":{"id":"5ee4ceb1"},"source":["### 2. **Classifier 1: Bag-of-Words (BoW) Baseline**\n","\n","We will reuse the TF-IDF feature extraction pipeline combined with Logistic Regression. This demonstrates how well a classical, linear model performs without needing to understand the sequence or context of the words."]},{"cell_type":"code","execution_count":null,"id":"301de865","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"301de865","executionInfo":{"status":"ok","timestamp":1764434618049,"user_tz":0,"elapsed":29093,"user":{"displayName":"Rahul Mourya","userId":"07975233105233353508"}},"outputId":"373952d0-7287-41fa-b1ae-134e5425fd1d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Starting BoW (TF-IDF) Classification...\n","\n","--- BoW Classifier Performance ---\n","Test Accuracy: 0.9013\n","Weighted F1-Score: 0.9013\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    negative       0.91      0.89      0.90      5000\n","    positive       0.89      0.91      0.90      5000\n","\n","    accuracy                           0.90     10000\n","   macro avg       0.90      0.90      0.90     10000\n","weighted avg       0.90      0.90      0.90     10000\n","\n"]}],"source":["# %%\n","# --- Bag-of-Words Model (TF-IDF + Logistic Regression) ---\n","\n","print(\"Starting BoW (TF-IDF) Classification...\")\n","\n","# 1. Feature Engineering: TfidfVectorizer\n","# min_df=5: Ignore terms that appear in less than 5 documents.\n","# stop_words='english': Remove common English stopwords.\n","tfidf_vectorizer = TfidfVectorizer(\n","    min_df=5,\n","    stop_words='english',\n","    ngram_range=(1, 2) # Also include 2-word combinations (bigrams)\n",")\n","\n","# 2. Classifier: Logistic Regression\n","log_reg_classifier = LogisticRegression(\n","    solver='lbfgs',\n","    random_state=SEED,\n","    max_iter=1000\n",")\n","\n","# 3. Build a pipeline: chain vectorization and classification\n","bow_pipeline = Pipeline([\n","    ('tfidf', tfidf_vectorizer),\n","    ('clf', log_reg_classifier)\n","])\n","\n","# Training\n","bow_pipeline.fit(X_train, y_train)\n","\n","# Prediction\n","y_pred_bow = bow_pipeline.predict(X_test)\n","\n","# Evaluation\n","accuracy_bow = accuracy_score(y_test, y_pred_bow)\n","f1_bow = f1_score(y_test, y_pred_bow, average='weighted')\n","\n","print(\"\\n--- BoW Classifier Performance ---\")\n","print(f\"Test Accuracy: {accuracy_bow:.4f}\")\n","print(f\"Weighted F1-Score: {f1_bow:.4f}\")\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_test, y_pred_bow, target_names=target_names))\n","\n","# Store results for final comparison\n","results = {'BoW (TF-IDF)': {'Accuracy': accuracy_bow, 'F1-Score': f1_bow}}\n","# %%\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}