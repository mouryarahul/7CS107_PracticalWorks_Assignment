{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6aa3b70",
   "metadata": {},
   "source": [
    "## Tutorial: GRU Language Model with Subword Tokenization (BPE)\n",
    " \n",
    "**Target Audience:** MSc Computer Science Students  \n",
    "**Topic:** NLP, Recurrent Neural Networks, Tokenization\n",
    "\n",
    "## 1. Introduction\n",
    " \n",
    "In the previous tutorial, we predicted text character-by-character. While simple, character-level models suffer from long sequence lengths (vanishing gradients) and lack semantic density in individual inputs.\n",
    "\n",
    "In this tutorial, we advance to **Subword Tokenization** using **Byte-Pair Encoding (BPE)**.\n",
    " \n",
    "### Why Subword Tokenization?\n",
    "1.  **Efficiency:** Sequences are much shorter than character sequences, allowing the GRU to capture context over a longer text span.\n",
    "2.  **Open Vocabulary:** Unlike strict word-level models (which fail on \"Unknown\" words), subword models can construct unknown words from known sub-parts (e.g., \"unfriendly\" $\\rightarrow$ \"un\", \"friend\", \"ly\").\n",
    "\n",
    "## 2. Setup and Dependencies\n",
    " \n",
    "We will use the `tokenizers` library from Hugging Face for efficient BPE training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6245de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tokenizers\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers, decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fa579e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# --- CRITICAL IMPORTS FOR TPU/XLA ---\n",
    "# Note: torch_xla is only needed for Google Cloud TPU environments\n",
    "# This notebook will work fine without it on CPU/CUDA/MPS\n",
    "XLA_AVAILABLE = False\n",
    "xm = None\n",
    "\n",
    "# Uncomment the following lines if running on TPU:\n",
    "# try:\n",
    "#     import torch_xla.core.xla_model as xm\n",
    "#     XLA_AVAILABLE = True\n",
    "# except ImportError:\n",
    "#     XLA_AVAILABLE = False\n",
    "#     print(\"WARNING: torch_xla not found. Running on CPU/CUDA fallback.\")\n",
    "# --- END XLA IMPORTS ---\n",
    "\n",
    "# Set device for PyTorch operations\n",
    "if XLA_AVAILABLE:\n",
    "    # Use xm.xla_device() to get the primary TPU core device\n",
    "    DEVICE = xm.xla_device()\n",
    "    N_DEVICES = 1 # Force single device count\n",
    "    print(f\"Using Single XLA Device: {DEVICE}\")\n",
    "elif torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    DEVICE = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device('mps')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "print(f'Using device: {DEVICE}')\n",
    "\n",
    "\n",
    "# Uncomment the following lines if running on Google Colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8afe76",
   "metadata": {},
   "source": [
    "\n",
    "## 3. The Dataset & Tokenizer Training\n",
    " \n",
    "#### We use the same short stories dataset. However, we must now **train** a tokenizer to learn the most frequent subword patterns in this specific text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0fa213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_TEXT = \"\"\"\n",
    "Title: The Wolf and the Lamb.\n",
    "Wolf, meeting with a Lamb astray from the fold, resolved not to lay violent hands on him, but to find some plea to justify to the Lamb the Wolf's right to eat him. \n",
    "He thus addressed him: \"Sirrah, last year you grossly insulted me.\" \n",
    "\"Indeed,\" bleated the Lamb in a mournful tone of voice, \"I was not then born.\" \n",
    "Then said the Wolf, \"You feed in my pasture.\" \n",
    "\"No, good sir,\" replied the Lamb, \"I have not yet tasted grass.\" \n",
    "Again said the Wolf, \"You drink of my well.\" \n",
    "\"No,\" exclaimed the Lamb, \"I never yet drank water, for as yet my mother's milk is both food and drink to me.\n",
    "\" Upon which the Wolf seized him and ate him up, saying, \"Well! I won't remain supperless, even though you refute every one of my imputations.\n",
    "\" The tyrant will always find a pretext for his tyranny.\n",
    "\n",
    "Title: The Bat and the Weasels.\n",
    "A Bat who fell upon the ground and was caught by a Weasel pleaded to be spared his life. \n",
    "The Weasel refused, saying that he was by nature the enemy of all birds. \n",
    "The Bat assured him that he was not a bird, but a mouse, and thus was set free. \n",
    "Shortly afterwards the Bat again fell to the ground and was caught by another Weasel, whom he likewise entreated not to eat him. \n",
    "The Weasel said that he had a special hostility to mice. \n",
    "The Bat assured him that he was not a mouse, but a bird, and thus escaped. \n",
    "It is wise to turn circumstances to good account.\n",
    "\n",
    "Title: The Ass and the Grasshopper.\n",
    "An Ass having heard some Grasshoppers chirping, was highly enchanted; and, desiring to possess the like charms of melody, demanded what sort of food they lived on to give them such beautiful voices. \n",
    "They replied, \"The dew.\" The Ass resolved that he would live only upon dew, and in a short time died of hunger.\n",
    "\n",
    "Title: The Lion and the Mouse.\n",
    "A Lion was awakened from sleep by a Mouse running over his face. \n",
    "Rising up angrily, he caught him and was about to kill him, when the Mouse piteously entreated, saying: \"If you would only spare my life, I would be sure to repay your kindness.\" \n",
    "The Lion laughed and let him go. \n",
    "It happened shortly after this that the Lion was caught by some hunters, who bound him by strong ropes to the ground. \n",
    "The Mouse, recognizing his roar, came and gnawed the rope with his teeth, and set him free, exclaiming: \"You ridiculed the idea of my ever being able to help you, not expecting to receive from me any repayment of your favor; now you know that it is possible for even a Mouse to con benefits on a Lion.\"\n",
    "\n",
    "Title: The Gruffalo.\n",
    "A mouse took a stroll through the deep dark wood.\n",
    "A fox saw the mouse, and the mouse looked good.\n",
    "\"Where are you going to, little brown mouse?\n",
    "Come and have lunch in my underground house.”\n",
    "\n",
    "\"It's terribly kind of you, Fox, but no- I'm going to have lunch with a Gruffalo.\"\n",
    "\"A Gruffalo? What's a Gruffalo?\"\n",
    "A Gruffalo! Why, didn't you know?\n",
    "“He has terrible tusks,\n",
    "and terrible claws, and terrible teeth in his terrible jaws.\"\n",
    "\"Where are you meeting him?\"\n",
    "\"Here, by these rocks, And his favourite food is roasted fox.\"\n",
    "\"Roasted fox! I'm off!\" Fox said.\n",
    "\"Goodbye, little mouse,\" and away he sped.\n",
    "\n",
    "\"Silly old Fox! Doesn't he know\n",
    "There's no such thing as a Gruffalo?\"\n",
    "\n",
    "On went the mouse through the deep dark\n",
    "wood. An owl saw the mouse, and the mouse looked good.\n",
    "\"Where are you going to, little brown mouse?\n",
    "Come and have tea in my treetop house.\"\n",
    "\"It's terribly kind of you, Owl, but no\n",
    "I'm going to have tea with a Gruffalo.\"\n",
    "\"A Gruffalo? What's a Gruffalo?\"\n",
    "\"A Gruffalo! Why, didn't you know?\n",
    "He has knobbly knees,and turned out toes,\n",
    "\"Where are you meeting him?\"\n",
    "\"Here, by this stream,\n",
    "And his favourite food is owl ice cream.\"\n",
    "\"Owl ice cream! Too whit too whoo!\"\n",
    "\"Goodbye, little mouse,\"\n",
    "and away Owl flew.\n",
    "\"Silly old Owl! Doesn't he know,\n",
    "There's no such thing as a Gruffalo?\"\n",
    "\n",
    "On went the mouse through the deep dark wood.\n",
    "A snake saw the mouse, and the mouse looked good.\n",
    "\"Where are you going to, little brown mouse?\n",
    "Come for a feast in my log pile house.\"\n",
    "\"It's wonderfully good of you, Snake, but no I'm having a feast with a Gruffalo.\"\n",
    "\"A Gruffalo? What's a Gruffalo?\"\n",
    "\" A Gruffalo! Why, didn't you know?\n",
    "His eyes are orange, his tongue is black,\n",
    "He has purple prickles all over his back.\"\n",
    "\"Where are you meeting him?\"\n",
    "\"Here, by this lake,\n",
    "And his favourite food is scrambled snake.\"\n",
    "\"Scrambled snake! It's time I hid!\"\n",
    "\"Goodbye, little mouse,\"\n",
    "and away Snake slid.\n",
    "\"Silly old Snake! Doesn't he know,\n",
    "There's no such thing as a Gruffal…..?“Oh!”\n",
    "\n",
    "But who is this creature with terrible claws\n",
    "And terrible teeth in his terrible jaws?\n",
    "He has knobbly knees, and turned\n",
    "out toes,\n",
    "And a poisonous wart at the end of his nose.\n",
    "His eyes are orange, his tongue is black,\n",
    "He has purple prickles all over his back.\n",
    "\"Oh help! Oh no! It's a Gruffalo!\n",
    "\n",
    "\"My favourite food !\" the Gruffalo\n",
    "\"You'll taste good on a slice of bread!\"\n",
    "\"Good?\" said the mouse. \"Don't call me good!\n",
    "I'm the scariest creature in this wood.\n",
    "Just walk behind me and soon you'll see,\n",
    "Everyone is afraid of me.\"\n",
    "\"All right,\" said the Gruffalo, bursting with laughter.\n",
    "\"You go ahead and I'll follow after.\"\n",
    "They walked and walked till the Gruffalo said,\n",
    "\"I hear a hiss in the leaves ahead.\"\n",
    "\n",
    "\"It's Snake,\" said the mouse.\n",
    "\"Why, Snake,hello!\"\n",
    "Snake took one look at the Gruffalo.\n",
    "\"Oh crumbs!\" he said, \"Goodbye, little mouse!\"\n",
    "And off he slid to his log pile house.\n",
    "\"You see?\" said the mouse. \"I told you so.\"\n",
    "\"Amazing!\" said the Gruffalo.\n",
    "They walked some more till the Gruffalo said,\n",
    "“I hear a hoot in the trees ahead.\"\n",
    "\n",
    "\"It's Owl,\" said the mouse. \"Why, Owl, hello!\"\n",
    "Owl took one look at the Gruffalo.\n",
    "\"Oh dear!\" he said, \"Goodbye, little mouse!\" \n",
    "And off he flew to his treetop house.\n",
    "\"You see?\" said the mouse. \"I told you so.\"\n",
    "\"Astounding!\" said the Gruffalo.\n",
    "They walked some more till\n",
    "the Gruffalo said,\n",
    "\"I can hear feet on the path ahead.\"\n",
    "\n",
    "\"It's Fox,\" said the mouse.\n",
    "\"Why, Fox, hello!\"\n",
    "Fox took one look at the Gruffalo.\n",
    "\"Oh help!\" he said, \"Goodbye, little mouse!\"\n",
    "And off he ran to his underground house.\n",
    "\"Well, Gruffalo,\" said the mouse. \"You see?\n",
    "Everyone is afraid of me!\n",
    "But now my tummy's beginning to rumble.\n",
    "My favourite food is Gruffalo crumble!\"\n",
    "\"Gruffalo crumble!\" the Gruffalo said,\n",
    "And quick as the wind he turned and fled.\n",
    "\n",
    "All was quiet in the deep dark wood.\n",
    "The mouse found a nut and the nut was good. The End.\n",
    "\n",
    "Title: Twinkle, Twinkle Little Star.\n",
    "Twinkle, twinkle, little star,\n",
    "How I wonder what you are,\n",
    "Up above the world so high,\n",
    "Like a diamond in the sky, twinkle, twinkle, little star,\n",
    "How I wonder what you are?\n",
    "\n",
    "Title: I'm a Little Tea Pot.\n",
    "I’m a little teapot, short and stout\n",
    "Here’s my handle (place hand on hip)\n",
    "Here’s my spout (stick your other arm out straight)\n",
    "When I get all steamed up, hear me shout\n",
    "Just tip me over and pour me out (lean over with your spout arm.\n",
    "\n",
    "Title: London Bridge is Falling Down (Short Version)\n",
    "London Bridge is falling down,\n",
    "Falling down, falling down.\n",
    "London Bridge is falling down,\n",
    "My fair lady.\n",
    "\n",
    "Build it up with wood and clay,\n",
    "Wood and clay, wood and clay,\n",
    "Build it up with wood and clay,\n",
    "My fair lady.\n",
    "\n",
    "Wood and clay will wash away,\n",
    "Wash away, wash away,\n",
    "Wood and clay will wash away,\n",
    "My fair lady.\n",
    "\n",
    "Build it up with iron and steel,\n",
    "Iron and steel, iron and steel,\n",
    "Build it up with iron and steel,\n",
    "My fair lady.\n",
    "\n",
    "Iron and steel will bend and bow,\n",
    "Bend and bow, bend and bow,\n",
    "Iron and steel will bend and bow,\n",
    "My fair lady.\n",
    "\n",
    "Build it up with silver and gold,\n",
    "Silver and gold, silver and gold,\n",
    "Build it up with silver and gold,\n",
    "My fair lady.\n",
    "\n",
    "Title: Mary Had a Little Lamb.\n",
    "Mary had a little lamb,\n",
    "His fleece was white as snow,\n",
    "And everywhere that Mary went,\n",
    "The lamb was sure to go\n",
    "\n",
    "He followed her to school one day,\n",
    "Which was against the rule,\n",
    "It made the children laugh and play,\n",
    "To see a lamb at school.\n",
    "\n",
    "And so the teacher turned him out,\n",
    "But still he lingered near,\n",
    "And waited patiently about,\n",
    "Till Mary did appear.\n",
    "\n",
    "\"What makes the lamb love Mary so?\"\n",
    "The eager children cry;\n",
    "\"Why, Mary loves the lamb, you know,\"\n",
    "The teacher did reply.\n",
    "\n",
    "Title: Humpty Dumpty.\n",
    "Humpty Dumpty sat on a wall,\n",
    "Humpty Dumpty had a great fall,\n",
    "All the king’s horses and all the king’s men,\n",
    "Couldn’t put Humpty together again.\n",
    "\n",
    "Title: Hey Diddle Diddle, Mother Goose.\n",
    "Hey diddle diddle, the cat and the fiddle,\n",
    "The cow jumped over the moon.\n",
    "The little dog laughed to see such fun\n",
    "And the dish ran away with the spoon!\n",
    "\n",
    "Title: Baa Baa Black Sheep.\n",
    "Baa baa black sheep, have you any wool?\n",
    "Yes sir, yes sir, three bags full!\n",
    "One for the master, one for the dame,\n",
    "And one for the little boy who lives down the lane.\n",
    "\n",
    "Title: One, Two, Three, Four.\n",
    "One, two, three, four, five\n",
    "Once I caught a fish alive.\n",
    "Six, seven, eight, nine, ten\n",
    "Then I let it go again.\n",
    "Why did you let it go?\n",
    "Because it bit my finger so.\n",
    "Which finger did it bite?\n",
    "This little finger on my right.\n",
    "\n",
    "Title: Hickory Dickory Dock.\n",
    "Hickory dickory dock (Gently bounce baby to the beat)\n",
    "The mouse ran up the clock (run your fingers from your baby's toes to their chin)\n",
    "The clock struck one (clap once)\n",
    "The mouse ran down (run your fingers down to your baby's toes)\n",
    "Hickory dickory dock.\n",
    "\n",
    "Hickory dickory dock (Gently bounce baby to the beat)\n",
    "The mouse ran up the clock (run your fingers from your baby's toes to their chin)\n",
    "The clock struck two (clap twice)\n",
    "The mouse went \"boo!\" (cover baby's eyes with your hands then pull them away on boo!)\n",
    "Hickory dickory dock.\n",
    "\n",
    "Three… the mouse went weeee (lift baby in the air on weeee)\n",
    "Four…The mouse went \"no more!\" (shake your finger no more!)\n",
    "\n",
    " \n",
    "Title: Polly Put the Kettle On.\n",
    "Polly put the kettle on,\n",
    "Polly put the kettle on,\n",
    "Polly put the kettle on,\n",
    "We’ll all have tea.\n",
    "\n",
    "Sukey take it off again,\n",
    "Sukey take it off again,\n",
    "Sukey take it off again,\n",
    "They’ve all gone away.\n",
    "    \n",
    "Pop! Goes the Weasel.\n",
    "Half a pound of tuppenny rice,\n",
    "Half a pound of treacle,\n",
    "That’s the way the money goes,\n",
    "Pop! goes the weasel.\n",
    "\n",
    "Up and down the City road,\n",
    "In and out the Eagle,\n",
    "That’s the way the money goes,\n",
    "Pop! goes the weasel.\n",
    "\n",
    "Title: Ring-a-Ring O’Roses.\n",
    "Ring-a-ring o’roses\n",
    "A pocketful of posies\n",
    "Atishoo, atishoo\n",
    "We all fall down.\n",
    "\n",
    "Title: Jack and Jill.\n",
    "Jack and Jill went up the hill\n",
    "To fetch a pail of water.\n",
    "Jack fell down and broke his crown,\n",
    "And Jill came tumbling after.\n",
    "\n",
    "Up Jack got, and home did trot,\n",
    "As fast as he could caper,\n",
    "He went to bed to mend his head,\n",
    "With vinegar and brown paper.\n",
    "    \n",
    "Title: This Old Man.\n",
    "This old man, he played one\n",
    "He played knick-knack on my thumb\n",
    "With a knick knack paddywhack give the dog a bone\n",
    "This old man cam rolling home…\n",
    "\n",
    "Two… on my shoe\n",
    "Three… on my knee\n",
    "Four… on my door\n",
    "Five… on my hive\n",
    "Six… on my sticks\n",
    "Seven…up to heaven\n",
    "Eight… on my gate\n",
    "Nine… on my spine\n",
    "Ten… once again\n",
    "\n",
    "Title: Round and Round the Garden.\n",
    "Round and round the garden, like a Teddy Bear (draw a circle with your finger on baby’s palm)\n",
    "One step, two step, (walk your finger up baby’s arm)\n",
    "Tickle you under there! (tickle baby under the chin)\n",
    "\n",
    "Title: Sing a Song of Sixpence.\n",
    "Sing a song of sixpence a pocket full of rye,\n",
    "Four and twenty blackbirds baked in a pie,\n",
    "When the pie was opened the birds began to sing,\n",
    "Oh wasn't that a dainty dish to set before the king?\n",
    "\n",
    "The king was in his counting house counting out his money,\n",
    "The queen was in the parlour eating bread and honey,\n",
    "The maid was in the garden hanging out the clothes,\n",
    "When down came a blackbird and pecked off her nose!\n",
    "    \n",
    "Title: This Little Piggy.\n",
    "This little piggy went to market (touch baby’s biggest toe)\n",
    "This little piggy stayed at home (touch the next toe)\n",
    "This little piggy had roast beef (and the next)\n",
    "This little piggy had none (and the next)\n",
    "And this little piggy went...Wee wee wee all the way home... (touch the little toe and then run your hand up baby tickling gently as you go)\n",
    "\n",
    "Title: Little Miss Muffet.\n",
    "Little Miss Muffet sat on a tuffet,\n",
    "Eating her curds and whey,\n",
    "Along came a spider, who sat down beside her,\n",
    "And frightened Miss Muffet away!\n",
    "\n",
    "\n",
    "Title: Duke of York.\n",
    "Oh, the grand old Duke of York\n",
    "He had ten thousand men\n",
    "He marched them up to the top of the hill\n",
    "And he marched them down again\n",
    "\n",
    "And when they were up, they were up\n",
    "And when they were down, they were down\n",
    "And when they were only half-way up\n",
    "They were neither up nor down\n",
    "\n",
    "Oh, the grand old Duke of York\n",
    "He had ten thousand men\n",
    "He marched them up to the top of the hill\n",
    "And he marched them down again\n",
    "\n",
    "And when they were up, they were up\n",
    "And when they were down, they were down\n",
    "And when they were only half-way up\n",
    "They were neither up nor down\n",
    "\n",
    "Oh, the grand old Duke of York\n",
    "He had ten thousand men\n",
    "He marched them up to the top of the hill\n",
    "And he marched them down again\n",
    "\n",
    "And when they were up, they were up\n",
    "And when they were down, they were down\n",
    "And when they were only half-way up\n",
    "They were neither up nor down\n",
    "\n",
    "\n",
    "Title: Wheels on the Bus.\n",
    "The wheels on the bus go round and round\n",
    "Round and round\n",
    "Round and round\n",
    "The wheels on the bus go round and round\n",
    "All day long\n",
    "No, it started to rain\n",
    "Oh no, we need to make the wipers go swish, swish, swish\n",
    "Are you ready? Here we go\n",
    "The wipers on the bus go swish, swish, swish\n",
    "Swish, swish, swish\n",
    "Swish, swish, swish\n",
    "The wipers on the bus go swish, swish, swish\n",
    "All day long\n",
    "Wow, it suddenly got very noisy on the bus\n",
    "Lots of people have gone on and started to chat\n",
    "Are you ready?\n",
    "The people on the bus go chat, chat, chat\n",
    "Chat, chat, chat\n",
    "Chat, chat, chat\n",
    "The people on the bus go chat, chat, chat\n",
    "All day long\n",
    "Alright everyone, it's time to beat the horn on the bus\n",
    "Get ready with your 'uh, uh, uh'\n",
    "Here we go\n",
    "The horn on the bus goes beep, beep, beep\n",
    "Beep, beep, beep\n",
    "Beep, beep, beep\n",
    "The horn on the bus goes beep, beep, beep\n",
    "All day long\n",
    "Yeah, well done everyone\n",
    "Great singing\n",
    "Come on, let's ride the bus one more time\n",
    "Ready to sing? Here we go\n",
    "The wheels on the bus go round and round\n",
    "Round and round\n",
    "Round and round\n",
    "The wheels on the bus go round and round\n",
    "All day long\n",
    "\n",
    " \n",
    "Title: Little Bo Beep.\n",
    "Little Bo Peep has lost her sheep\n",
    "And doesn’t know where to find them;\n",
    "Leave them alone, and they’ll come home,\n",
    "Bringing their tails behind them.\n",
    " \n",
    "Little Bo Peep fell fast asleep\n",
    "And dreamt she heard them bleating;\n",
    "But when she awoke, she found it a joke,\n",
    "For they were still a-fleeting.\n",
    "\n",
    "Then up she took her little crook,\n",
    "Determined for to find them;\n",
    "She found them indeed, but it made her heart bleed,\n",
    "For they’d left their tales behind them.\n",
    " \n",
    "It happened one day, as Bo Peep did stray\n",
    "Into a meadow hard by,\n",
    "There she espied their tales side by side,\n",
    "All hung on a tree to dry.\n",
    "\n",
    "She heaved a sigh and wiped her eye,\n",
    "And over the hillocks went rambling,\n",
    "And tried what she could, as a shepherdess should,\n",
    "To tack each again to its lambkin.\n",
    "\n",
    "Title: I’m a Little Teapot.\n",
    "I'm a little teapot,\n",
    "Short and stout,\n",
    "Here is my handle\n",
    "Here is my spout\n",
    "When I get all steamed up,\n",
    "Hear me shout,\n",
    "Tip me over and pour me out!\n",
    "I'm a very special teapot,\n",
    "Yes, it's true,\n",
    "Here's an example of what I can do,\n",
    "I can turn my handle into a spout,\n",
    "Tip me over and pour me out!\n",
    "\n",
    "Title: If You’re Happy And You Know It.\n",
    "If you're happy and you know it clap your hands\n",
    "If you're happy and you know it clap your hands\n",
    "If you're happy and you know it and you really want to show it\n",
    "If you're happy and you know it clap your hands\n",
    "If you're happy and you know it turn around\n",
    "If you're happy and you know it turn around\n",
    "If you're happy and you know it and you really want to show it\n",
    "If you're happy and you know it turn around\n",
    " \n",
    "Title: Round and Round the Garden.\n",
    "Round and round the garden\n",
    "Like a teddy bear.\n",
    "One step, two step,\n",
    "Tickle you under there.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4adaf377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer trained. Vocab Size: 50257\n",
      "Sample: 'If you're happy and you know it turn around.'\n",
      "IDs: [1532, 345, 821, 3772, 290, 345, 760, 340, 1210, 1088, 13]\n",
      "Tokens: ['If', 'Ġyou', \"'re\", 'Ġhappy', 'Ġand', 'Ġyou', 'Ġknow', 'Ġit', 'Ġturn', 'Ġaround', '.']\n"
     ]
    }
   ],
   "source": [
    "# Save text to a temp file for the tokenizer trainer\n",
    "\n",
    "# Uncommented the following lines if running on Google Colab\n",
    "#FILEPATH = \"/content/drive/MyDrive/Colab Notebooks/corpus.txt\"\n",
    "FILEPATH = \"corpus.txt\"\n",
    "\n",
    "with open(FILEPATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(RAW_TEXT)\n",
    "\n",
    "# --- Build BPE Tokenizer ---\n",
    "# 1. Initialize a BPE model\n",
    "#tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "# 2. Pre-tokenization\n",
    "# We use Punctuation pre-tokenizer. This splits text at punctuation marks\n",
    "# but does not split on whitespace. This is important for preserving spaces\n",
    "# as distinct characters in the BPE model.\n",
    "# By avoiding whitespace splitting, the BPE model treats the space character ' '\n",
    "# as a literal character. It will learn ' ' as a distinct token or merge it \n",
    "# into words (e.g., \"The\" + \" \" -> \"The \").\n",
    "\n",
    "#tokenizer.pre_tokenizer = pre_tokenizers.Punctuation()\n",
    "#tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "\"\"\"\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Sequence([\n",
    "    pre_tokenizers.BertPreTokenizer(),\n",
    "    pre_tokenizers.Punctuation(),\n",
    "])\n",
    "\"\"\"\n",
    "\n",
    "# 3. Trainer: Learn the vocabulary\n",
    "# vocab_size=500 is small, but suitable for this tiny dataset.\n",
    "#trainer = trainers.BpeTrainer(vocab_size=2000, special_tokens=[\"[UNK]\", \"[PAD]\"])\n",
    "\n",
    "# 4. Train\n",
    "#tokenizer.train([\"corpus.txt\"], trainer)\n",
    "\n",
    "# 5. Add a decoder to merge subwords back into text later\n",
    "#tokenizer.decoder = decoders.BPEDecoder()\n",
    "\n",
    "# --- Load Pre-trained GPT-2 Tokenizer ---\n",
    "# We fetch the tokenizer definition directly from Hugging Face Hub.\n",
    "# This tokenizer uses a Byte-Level BPE model.\n",
    "try:\n",
    "    tokenizer = Tokenizer.from_pretrained(\"gpt2\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading GPT-2 tokenizer. Ensure internet access or local file. {e}\")\n",
    "    # Fallback/Exit handling would go here in production\n",
    "    raise e\n",
    "\n",
    "\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print(f\"Tokenizer trained. Vocab Size: {vocab_size}\")\n",
    "\n",
    "# Test encoding\n",
    "sample = \"If you're happy and you know it turn around.\"\n",
    "encoded = tokenizer.encode(sample)\n",
    "print(f\"Sample: '{sample}'\")\n",
    "print(f\"IDs: {encoded.ids}\")\n",
    "print(f\"Tokens: {encoded.tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f3f8b0",
   "metadata": {},
   "source": [
    "## 4. Preparing the Data for PyTorch\n",
    " \n",
    "### Now we convert the entire text into a sequence of token IDs.\n",
    "\n",
    "**Difference from Character-Level:**\n",
    "* **Input:** Sequence of Subword IDs.\n",
    "* **Output:** The next Subword ID.\n",
    "* **Sequence Length:** We can use a shorter `seq_len` (e.g., 10-20) because each token represents more information than a single character.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf2d8738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in dataset: 4536\n",
      "Number of batches: 280\n",
      "Input batch shape: torch.Size([16, 50])\n",
      "Target batch shape: torch.Size([16, 50])\n",
      "First input sequence IDs: [13, 198, 198, 19160, 25, 770, 5706, 1869, 13, 198, 1212, 1468, 582, 11, 339, 2826, 530, 198, 1544, 2826, 638, 624, 12, 15418, 441, 319, 616, 15683, 198, 3152, 257, 638, 624, 47868, 279, 13218, 1929, 441, 1577, 262, 3290, 257, 9970, 198, 1212, 1468, 582, 12172, 10708, 1363]\n",
      "First target sequence IDs: [198, 198, 19160, 25, 770, 5706, 1869, 13, 198, 1212, 1468, 582, 11, 339, 2826, 530, 198, 1544, 2826, 638, 624, 12, 15418, 441, 319, 616, 15683, 198, 3152, 257, 638, 624, 47868, 279, 13218, 1929, 441, 1577, 262, 3290, 257, 9970, 198, 1212, 1468, 582, 12172, 10708, 1363, 1399]\n",
      "First input sequence Tokens: ['.', 'Ċ', 'Ċ', 'Title', ':', 'ĠThis', 'ĠOld', 'ĠMan', '.', 'Ċ', 'This', 'Ġold', 'Ġman', ',', 'Ġhe', 'Ġplayed', 'Ġone', 'Ċ', 'He', 'Ġplayed', 'Ġkn', 'ick', '-', 'kn', 'ack', 'Ġon', 'Ġmy', 'Ġthumb', 'Ċ', 'With', 'Ġa', 'Ġkn', 'ick', 'Ġknack', 'Ġp', 'addy', 'wh', 'ack', 'Ġgive', 'Ġthe', 'Ġdog', 'Ġa', 'Ġbone', 'Ċ', 'This', 'Ġold', 'Ġman', 'Ġcam', 'Ġrolling', 'Ġhome']\n",
      "First target sequence Tokens: ['Ċ', 'Ċ', 'Title', ':', 'ĠThis', 'ĠOld', 'ĠMan', '.', 'Ċ', 'This', 'Ġold', 'Ġman', ',', 'Ġhe', 'Ġplayed', 'Ġone', 'Ċ', 'He', 'Ġplayed', 'Ġkn', 'ick', '-', 'kn', 'ack', 'Ġon', 'Ġmy', 'Ġthumb', 'Ċ', 'With', 'Ġa', 'Ġkn', 'ick', 'Ġknack', 'Ġp', 'addy', 'wh', 'ack', 'Ġgive', 'Ġthe', 'Ġdog', 'Ġa', 'Ġbone', 'Ċ', 'This', 'Ġold', 'Ġman', 'Ġcam', 'Ġrolling', 'Ġhome', 'âĢ¦']\n"
     ]
    }
   ],
   "source": [
    "# Encode entire corpus\n",
    "full_encoding = tokenizer.encode(RAW_TEXT)\n",
    "data_ids = torch.tensor(full_encoding.ids, dtype=torch.long)\n",
    "\n",
    "print(f\"Total tokens in dataset: {len(data_ids)}\")\n",
    "\n",
    "class SubwordDataset(Dataset):\n",
    "    def __init__(self, data_tensor, seq_len):\n",
    "        self.data_tensor = data_tensor\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_tensor) - self.seq_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Input: tokens [0, 1, ... N]\n",
    "        input_seq = self.data_tensor[idx : idx + self.seq_len]\n",
    "        # Target: tokens [1, 2, ... N+1]\n",
    "        target_seq = self.data_tensor[idx + 1 : idx + self.seq_len + 1]\n",
    "        return input_seq, target_seq\n",
    "\n",
    "# Hyperparameters\n",
    "SEQ_LEN = 50  # Context window (approx 10-15 words)\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "dataset = SubwordDataset(data_ids, SEQ_LEN)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "print(f\"Number of batches: {len(dataloader)}\")\n",
    "\n",
    "# Verify a batch\n",
    "for x_batch, y_batch in dataloader:\n",
    "    print(\"Input batch shape:\", x_batch.shape)  # Expected: (BATCH_SIZE, SEQ_LEN)\n",
    "    print(\"Target batch shape:\", y_batch.shape)  # Expected: (BATCH_SIZE, SEQ_LEN)\n",
    "    print(\"First input sequence IDs:\", x_batch[0].tolist())\n",
    "    print(\"First target sequence IDs:\", y_batch[0].tolist())\n",
    "    print(\"First input sequence Tokens:\", [tokenizer.id_to_token(id.item()) for id in x_batch[0]])\n",
    "    print(\"First target sequence Tokens:\", [tokenizer.id_to_token(id.item()) for id in y_batch[0]])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2babe274",
   "metadata": {},
   "source": [
    "## 5. GRU Model Implementation\n",
    " \n",
    "### The architecture remains similar, but the `vocab_size` is now larger (from ~60 chars to 500 subwords), and the `embedding_dim` handles semantic vectorization of these subwords.\n",
    "\n",
    "### $$ h_t = \\text{GRU}(x_t, h_{t-1}) $$\n",
    "### $$ y = \\text{Linear}(h_t) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ff27893",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUSubwordLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1, dropout=0.1):\n",
    "        super(GRUSubwordLM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Padding index is 1 because [UNK]=0, [PAD]=1 in this tokenizer setup usually,\n",
    "        # but strictly we should check tokenizer.token_to_id(\"[PAD]\").\n",
    "        # For this simple loop, we don't strictly need padding logic as we drop_last=True\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        # x: (batch, seq_len)\n",
    "        embeds = self.embedding(x) # (batch, seq, embed_dim)\n",
    "        \n",
    "        # output: (batch, seq, hidden)\n",
    "        output, hidden = self.gru(embeds, hidden)\n",
    "        \n",
    "        # Flatten for classification\n",
    "        # We reshape to (batch * seq, hidden)\n",
    "        output = output.reshape(-1, self.hidden_dim)\n",
    "        \n",
    "        # Project to vocab\n",
    "        logits = self.fc(output)\n",
    "        \n",
    "        return logits, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c024ffb4",
   "metadata": {},
   "source": [
    "## 6. Training\n",
    " \n",
    "### We train using Cross Entropy Loss. Because we are using subwords, the loss represents the perplexity of predicting the next subword.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe37753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUSubwordLM(\n",
      "  (embedding): Embedding(50257, 128)\n",
      "  (gru): GRU(128, 256, num_layers=2, batch_first=True, dropout=0.15)\n",
      "  (fc): Linear(in_features=256, out_features=50257, bias=True)\n",
      ")\n",
      "GRU Subword LM Model Parameters: 20,040,145\n"
     ]
    }
   ],
   "source": [
    "# Model Hyperparameters\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.15\n",
    "LR = 0.001\n",
    "GRAD_THRESH = 1.0\n",
    "EPOCHS = 10\n",
    "\n",
    "# Instantiate Model, Loss, and Optimizer\n",
    "model = GRUSubwordLM(vocab_size, EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS, dropout=DROPOUT).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "print(model)\n",
    "\n",
    "# Calculate total trainable parameters\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"GRU Subword LM Model Parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15929ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 5 | Loss: 0.3293\n",
      "Epoch 10 | Loss: 0.0926\n",
      "Done. Time: 164.51s\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "loss_history = []\n",
    "model.train()\n",
    "\n",
    "print(\"Starting training...\")\n",
    "start_t = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "        # Important: Since batches are shuffled and not sequential,\n",
    "        # we must initialize a fresh hidden state for each batch.\n",
    "        # We cannot carry over state from a previous unrelated batch.\n",
    "        h = model.init_hidden(BATCH_SIZE)\n",
    "        \n",
    "        # Detach hidden state\n",
    "        h = h.detach()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, h = model(x, h)\n",
    "\n",
    "        # Target shape must be flat: (batch * seq)\n",
    "        loss = criterion(output, y.view(-1))\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), GRAD_THRESH)\n",
    "\n",
    "        if XLA_AVAILABLE:\n",
    "            # XLA specific optimization step\n",
    "            xm.optimizer_step(optimizer)\n",
    "            xm.mark_step() # Signal end of computation step to XLA\n",
    "        else:\n",
    "            optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    loss_history.append(avg_loss)\n",
    "    \n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1} | Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(f\"Done. Time: {time.time()-start_t:.2f}s\")\n",
    "\n",
    "# Save the model\n",
    "# Use xm.save if XLA is available, otherwise standard torch.save\n",
    "save_model = xm.save if XLA_AVAILABLE else torch.save\n",
    "save_model(model.state_dict(), \"gru_subword_lm.pth\")\n",
    "\n",
    "# Plot loss history\n",
    "#plt.figure(figsize=(8,5))\n",
    "#plt.plot(loss_history)\n",
    "#plt.title(\"Training Loss (Subword Level)\")\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad0c208",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Generation\n",
    " \n",
    "### We generate tokens one by one and decode them using the BPE decoder.\n",
    "\n",
    "### **Process:**\n",
    "### 1. Tokenize prompt string -> `[IDs]`\n",
    "### 2. Feed `[IDs]` to GRU, get hidden state.\n",
    "### 3. Predict next ID.\n",
    "### 4. Append next ID to sequence.\n",
    "### 5. Decode complete sequence -> String."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e738cb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generated Text ---\n",
      "Title: The Loin a his favourite food is roasted fox.\"\n",
      "\"Roasted fox! I'm off!\" Fox said.\n",
      "\"Goodbye, little mouse,\" and away he sped.\n",
      "\n",
      "\"Silly old Fox! Doesn't he know\n",
      "There's no such thing as a Gruffalo?\"\n",
      "\n",
      "\n",
      "\n",
      "--- Generated Text (More Creative) ---\n",
      "Title: The Wolf and the Lamb.\n",
      "A Bat who fell upon the ground and was caught by a Weasel pleaded to be spared his life. \n",
      "The Weasel refused, saying that he was by nature the enemy of all birds. \n",
      "The Bat assured him that he was not a bird, but a mouse, and thus\n",
      "\n",
      "--- Generated Text (More Creative) ---\n",
      "Title: Hickory Dickory Dock.\n",
      "Hickory dickory dock (Gently bounce baby to the beat)\n",
      "The mouse ran up the clock (run your fingers from your baby's toes to their chin)\n",
      "The clock struck two (clap twice)\n",
      "The mouse went \"boo!\" (cover baby's eyes with\n",
      "\n",
      "--- Generated Text (More Creative) ---\n",
      "Title: The Gruffalo.\n",
      "A mouse took a stroll through the deep dark wood.\n",
      "A fox saw the mouse, and the mouse looked good.\n",
      "\"Where are you going to, little brown mouse?\n",
      "Come and have tea in my treetop house.\"\n",
      "\"It's terribly kind of you, Owl, but no\n",
      "I'm going to have tea with a Gruffalo.\"\n",
      "\"A Gruffalo? What's a Gruffalo?\"\n",
      "A Gruffalo! Why, didn't you know?\n",
      "“He has terrible tusks,\n",
      "and terrible claws, and terrible teeth in his terrible jaws.\"\n",
      "\"Where are you meeting him?\"\n",
      "\"Here, by these rocks, And his favourite food is roasted fox.\"\n",
      "\"Roasted fox! I'm off!\" Fox said.\n",
      "\"Goodbye, little mouse,\" and away he sped.\n",
      "\n",
      "\"Silly old Fox! Doesn't he know\n",
      "There's no such thing as a Gruffalo?\"\n",
      "\n",
      "On went the mouse through the deep dark wood.\n",
      "A snake saw the mouse, and the mouse looked good.\n",
      "\"Where are you going to, little brown mouse?\n",
      "Come and have tea in my treetop house.\"\n",
      "\"It's terribly kind of you, Owl,\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, tokenizer, seed_text=\"The Lion\", length=50, temp=0.8):\n",
    "    model.eval()\n",
    "    \n",
    "    # 1. Encode Seed\n",
    "    input_ids = tokenizer.encode(seed_text).ids\n",
    "    input_tensor = torch.tensor(input_ids).unsqueeze(0).to(DEVICE) # (1, seq_len)\n",
    "    \n",
    "    # Init hidden\n",
    "    hidden = model.init_hidden(1)\n",
    "    \n",
    "    generated_ids = input_ids.copy()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Priming: Run seed through to update hidden state\n",
    "        # We just want the state after the last token of the seed\n",
    "        _, hidden = model(input_tensor, hidden)\n",
    "        \n",
    "        # The input to the loop is the last token of the seed\n",
    "        last_token = input_tensor[:, -1].unsqueeze(1) # (1, 1)\n",
    "        \n",
    "        for _ in range(length):\n",
    "            # Forward pass\n",
    "            logits, hidden = model(last_token, hidden) # logits: (1, vocab_size)\n",
    "            \n",
    "            # Sampling\n",
    "            probs = torch.softmax(logits / temp, dim=1).squeeze()\n",
    "            next_token_id = torch.multinomial(probs, 1).item()\n",
    "            \n",
    "            generated_ids.append(next_token_id)\n",
    "            \n",
    "            # Update input\n",
    "            last_token = torch.tensor([[next_token_id]]).to(DEVICE)\n",
    "            \n",
    "    # Decode back to text\n",
    "    return tokenizer.decode(generated_ids)\n",
    "\n",
    "# Test Generation Part\n",
    "\n",
    "# Model Hyperparameters (must match training)\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.15\n",
    "\n",
    "# Instantiate and load the model\n",
    "model = GRUSubwordLM(vocab_size, EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS, dropout=DROPOUT).to(DEVICE)\n",
    "model.load_state_dict(torch.load(\"gru_subword_lm.pth\"))\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "print(\"--- Generated Text ---\")\n",
    "print(generate_text(model, tokenizer, seed_text=\"Title: The Loin\", length=60, temp=0.5))\n",
    "\n",
    "print(\"\\n--- Generated Text ---\")\n",
    "print(generate_text(model, tokenizer, seed_text=\"Title: The Wolf and the Lamb.\", length=60, temp=0.5))\n",
    "\n",
    "print(\"\\n--- Generated Text ---\")\n",
    "print(generate_text(model, tokenizer, seed_text=\"Title: Hickory Dickory Dock.\", length=60, temp=0.5))\n",
    "\n",
    "print(\"\\n--- Generated Text ---\")\n",
    "print(generate_text(model, tokenizer, seed_text=\"Title: The Gruffalo\", length=260, temp=0.5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
