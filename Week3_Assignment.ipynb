{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce808547",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mouryarahul/7CS107_PracticalWorks_Assignment/blob/master/Week3_Assignment.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4da3f53",
   "metadata": {},
   "source": [
    "\n",
    "# 7CS107 – Advanced AI & ML  \n",
    "## Week 3 Programming Assignment (100 marks)\n",
    "\n",
    "**Topic coverage:** Gradients & Hessians, convexity, Gradient Descent, Newton's Method, conditional probability, entropy, and mutual information.  \n",
    "**Input domain:** Primarily scalar-valued functions with **2D inputs**.  \n",
    "**What to submit:** Upload this completed notebook (with your code and outputs) to the VLE.\n",
    "\n",
    "> **Rules:**  \n",
    "> - You may use only **NumPy** and **Matplotlib**. Do **not** use SymPy, autograd, JAX, PyTorch, TensorFlow, scikit-learn, or other libraries that compute derivatives or information-theoretic quantities for you.  \n",
    "> - Write clean, vectorised code where possible.  \n",
    "> - Show intermediate outputs where asked.  \n",
    "> - Keep random seeds fixed where provided for reproducibility.\n",
    "\n",
    "**Marking breakdown:** Q1 (15), Q2 (15), Q3 (20), Q4 (15), Q5 (10), Q6 (12), Q7 (13) = **100 marks**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf08540",
   "metadata": {},
   "source": [
    "\n",
    "## Q1. Gradient and Hessian of a scalar-valued function (15 marks)\n",
    "\n",
    "Consider the scalar-valued function $f: \\mathbb{R}^2 \\rightarrow \\mathbb{R}$ defined by\n",
    "$f(x, y) = (x + 2y^3)^2 + 0.5x^2 + 3y^2$.\n",
    "\n",
    "1. (8 marks) Implement two functions:\n",
    "   - grad_f(xy) that returns the gradient $\\begin{bmatrix}\\dfrac{df}{dx}, \\dfrac{df}{dy}\\end{bmatrix}$.\n",
    "   - hess_f(xy) that returns the Hessian \n",
    "   $$\\begin{bmatrix}\n",
    "      & \\dfrac{d^2f}{dx^2} & \\dfrac{d^2f}{dxdy} \\\\\n",
    "      & \\dfrac{d^2f}{dydx} & \\dfrac{d^f}{dy^2}\n",
    "      \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   Do this by hand from first principles (use algebra and the chain/product rules). No symbolic/auto-diff.\n",
    "\n",
    "2. (7 marks) Verify your implementations numerically by comparing to finite-difference approximations of the gradient and Hessian at the points (0,0), (1,-1), (0.5,0.5).  \n",
    "   Report the max absolute error for each point (gradient and Hessian separately) using a small step (e.g., h=1e-5).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c363d8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def f(xy: np.ndarray) -> float:\n",
    "    # Scalar function f(x, y) = (x + 2*y^3)^2 + 0.5*x^2 + 3*y^2\n",
    "    x, y = float(xy[0]), float(xy[1])\n",
    "    return (x + 2*(y**3))**2 + 0.5*(x**2) + 3*(y**2)\n",
    "\n",
    "def grad_f(xy: np.ndarray) -> np.ndarray:\n",
    "    # Return gradient vector [df/dx, df/dy].\n",
    "    # ### BEGIN STUDENT CODE\n",
    "    raise NotImplementedError(\"Implement the analytical gradient here.\")\n",
    "    # ### END STUDENT CODE\n",
    "\n",
    "def hess_f(xy: np.ndarray) -> np.ndarray:\n",
    "    # Return Hessian matrix [[d2f/dx2, d2f/dxdy], [d2f/dydx, d2f/dy2]].\n",
    "    # ### BEGIN STUDENT CODE\n",
    "    raise NotImplementedError(\"Implement the analytical Hessian here.\")\n",
    "    # ### END STUDENT CODE\n",
    "\n",
    "def fd_grad(xy: np.ndarray, h: float = 1e-5) -> np.ndarray:\n",
    "    # Finite-difference gradient approximation (central difference)\n",
    "    e1 = np.array([1.0, 0.0])\n",
    "    e2 = np.array([0.0, 1.0])\n",
    "    g1 = (f(xy + h*e1) - f(xy - h*e1)) / (2*h)\n",
    "    g2 = (f(xy + h*e2) - f(xy - h*e2)) / (2*h)\n",
    "    return np.array([g1, g2])\n",
    "\n",
    "def fd_hess(xy: np.ndarray, h: float = 1e-4) -> np.ndarray:\n",
    "    # Finite-difference Hessian approximation (central differences)\n",
    "    e1 = np.array([1.0, 0.0])\n",
    "    e2 = np.array([0.0, 1.0])\n",
    "    f_xph = f(xy + h*e1); f_xmh = f(xy - h*e1)\n",
    "    f_yph = f(xy + h*e2); f_ymh = f(xy - h*e2)\n",
    "    f0 = f(xy)\n",
    "    d2f_dx2 = (f_xph - 2*f0 + f_xmh) / (h**2)\n",
    "    d2f_dy2 = (f_yph - 2*f0 + f_ymh) / (h**2)\n",
    "    f_pp = f(xy + h*e1 + h*e2)\n",
    "    f_pm = f(xy + h*e1 - h*e2)\n",
    "    f_mp = f(xy - h*e1 + h*e2)\n",
    "    f_mm = f(xy - h*e1 - h*e2)\n",
    "    d2f_dxdy = (f_pp - f_pm - f_mp + f_mm) / (4*h**2)\n",
    "    H = np.array([[d2f_dx2, d2f_dxdy],\n",
    "                  [d2f_dxdy, d2f_dy2]])\n",
    "    return H\n",
    "\n",
    "# --- Tests ---\n",
    "test_points = [np.array([0.0, 0.0]), np.array([1.0, -1.0]), np.array([0.5, 0.5])]\n",
    "for p in test_points:\n",
    "    try:\n",
    "        g_true = grad_f(p)\n",
    "        H_true = hess_f(p)\n",
    "    except NotImplementedError:\n",
    "        print(\"Implement grad_f and hess_f to run the tests.\")\n",
    "        break\n",
    "    g_fd = fd_grad(p, h=1e-5)\n",
    "    H_fd = fd_hess(p, h=1e-4)\n",
    "    print(f\"Point {p}:\")\n",
    "    print(\"  grad max|.| error:\", np.max(np.abs(g_true - g_fd)))\n",
    "    print(\"  Hess max|.| error:\", np.max(np.abs(H_true - H_fd)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddae412b",
   "metadata": {},
   "source": [
    "\n",
    "## Q2. Checking convexity via the Hessian (15 marks)\n",
    "\n",
    "A twice continuously differentiable function $f: \\mathbb{R}^2 \\rightarrow \\mathbb{R}$ is:\n",
    "- Convex if its Hessian is positive semi-definite (PSD) everywhere.\n",
    "- Strictly convex if the Hessian is positive definite (PD) everywhere.\n",
    "\n",
    "1. (9 marks) For each function below, write a function that returns its Hessian, then\n",
    "   check PD/PSD on a grid $\\mathcal{G}=\\{-2,-1,0,1,2\\}^2$ by inspecting eigenvalues:\n",
    "   - $f_1(x,y) = (x-1)^2 + 10(y-2)^2$  \n",
    "   - $f_2(x,y) = x^4 - 3x^2y + y^4$\n",
    "\n",
    "   Print, for each $f_i$, the minimum eigenvalue over $\\mathcal{G}$ and your conclusion (convex / strictly convex / non-convex).\n",
    "\n",
    "2. (6 marks) Briefly justify your conclusions in 3–5 lines based on the Hessian test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35805bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def hess_f1(xy: np.ndarray) -> np.ndarray:\n",
    "    # Hessian of f1(x,y) = (x-1)^2 + 10*(y-2)^2\n",
    "    # ### BEGIN STUDENT CODE\n",
    "    raise NotImplementedError(\"Implement Hessian for f1.\")\n",
    "    # ### END STUDENT CODE\n",
    "\n",
    "def hess_f2(xy: np.ndarray) -> np.ndarray:\n",
    "    # Hessian of f2(x,y) = x**4 - 3*x**2*y + y**4\n",
    "    # ### BEGIN STUDENT CODE\n",
    "    raise NotImplementedError(\"Implement Hessian for f2.\")\n",
    "    # ### END STUDENT CODE\n",
    "\n",
    "def min_eig_over_grid(hess_fun, grid_vals=(-2, -1, 0, 1, 2)) -> float:\n",
    "    mins = []\n",
    "    for x in grid_vals:\n",
    "        for y in grid_vals:\n",
    "            H = hess_fun(np.array([x, y], dtype=float))\n",
    "            lam_min = np.linalg.eigvalsh(H).min()\n",
    "            mins.append(lam_min)\n",
    "    return float(np.min(mins))\n",
    "\n",
    "# --- Tests/Reports ---\n",
    "try:\n",
    "    m1 = min_eig_over_grid(hess_f1)\n",
    "    print(\"f1: min eigenvalue over grid =\", m1)\n",
    "    # print(\"Conclusion: ...\")\n",
    "except NotImplementedError:\n",
    "    print(\"Implement hess_f1 to evaluate f1.\")\n",
    "\n",
    "try:\n",
    "    m2 = min_eig_over_grid(hess_f2)\n",
    "    print(\"f2: min eigenvalue over grid =\", m2)\n",
    "    # print(\"Conclusion: ...\")\n",
    "except NotImplementedError:\n",
    "    print(\"Implement hess_f2 to evaluate f2.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ab73f2",
   "metadata": {},
   "source": [
    "**Your written justification (3–5 lines):**\n",
    "\n",
    "*Write here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c71e87",
   "metadata": {},
   "source": [
    "\n",
    "## Q3. Implement Gradient Descent in 2D (20 marks)\n",
    "\n",
    "We will minimise the convex quadratic\n",
    "$g(x,y) = (x-1)^2 + 10(y-2)^2$.\n",
    "\n",
    "1. (10 marks) Implement gradient_descent_2d(g, grad_g, x0, step, tol, max_iter) that returns the iterate history as an array of shape (T,2). Stop when $||\\nabla g||_2 \\leq $ tol or max_iter is reached.\n",
    "2. (5 marks) Run with $x_0=[-2.0, -2.0]$, step=0.1, tol=1e-6, max_iter=500. Print: final point, final gradient norm, and number of iterations.\n",
    "3. (5 marks) Make a single 2D contour plot of g with the optimisation trajectory overlaid (markers at iterates).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78dc3a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implement grad_g and gradient_descent_2d to run this cell.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def g(xy: np.ndarray) -> float:\n",
    "    x, y = float(xy[0]), float(xy[1])\n",
    "    return (x-1.0)**2 + 10.0*(y-2.0)**2\n",
    "\n",
    "def grad_g(xy: np.ndarray) -> np.ndarray:\n",
    "    # Analytical gradient of g.\n",
    "    # ### BEGIN STUDENT CODE\n",
    "    raise NotImplementedError(\"Implement grad_g.\")\n",
    "    # ### END STUDENT CODE\n",
    "\n",
    "def gradient_descent_2d(fun, grad_fun, x0, step=0.1, tol=1e-6, max_iter=500):\n",
    "    # Basic Gradient Descent for R^2 -> R objectives.\n",
    "    # Returns: history array of shape (T, 2).\n",
    "    # ### BEGIN STUDENT CODE\n",
    "    raise NotImplementedError(\"Implement Gradient Descent.\")\n",
    "    # ### END STUDENT CODE\n",
    "\n",
    "# --- Run & report ---\n",
    "x0 = np.array([-2.0, -2.0], dtype=float)\n",
    "try:\n",
    "    hist = gradient_descent_2d(g, grad_g, x0, step=0.1, tol=1e-6, max_iter=500)\n",
    "    x_final = hist[-1]\n",
    "    print(\"Final x:\", x_final)\n",
    "    print(\"Final grad norm:\", np.linalg.norm(grad_g(x_final)))\n",
    "    print(\"Iterations:\", len(hist)-1)\n",
    "\n",
    "    # --- Plot (single plot only) ---\n",
    "    xs = np.linspace(-3, 3, 200)\n",
    "    ys = np.linspace(-3, 3, 200)\n",
    "    XX, YY = np.meshgrid(xs, ys)\n",
    "    ZZ = (XX-1.0)**2 + 10.0*(YY-2.0)**2\n",
    "    plt.figure()\n",
    "    plt.contour(XX, YY, ZZ, levels=30)\n",
    "    plt.plot(hist[:,0], hist[:,1], marker='o')\n",
    "    plt.title(\"Gradient Descent Trajectory on g(x,y)\")\n",
    "    plt.xlabel(\"x\"); plt.ylabel(\"y\")\n",
    "    plt.show()\n",
    "except NotImplementedError:\n",
    "    print(\"Implement grad_g and gradient_descent_2d to run this cell.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a50ca75",
   "metadata": {},
   "source": [
    "\n",
    "## Q4. Implement Newton's Method in 2D (15 marks)\n",
    "\n",
    "We will minimise the same convex function $g(x,y) = (x-1)^2 + 10(y-2)^2$ as in Q3.\n",
    "\n",
    "1. (10 marks) Implement newton_2d(g, grad_g, hess_g, x0, tol, max_iter) that uses the Newton step delta_x = -inv(H) * grad and returns the iterate history. Use full step size 1. If the Hessian is not invertible at any step, fall back to a small damped step (e.g. solve (H + lambda*I) * delta_x = -grad with lambda=1e-6).\n",
    "2. (5 marks) Compare to Gradient Descent from Q3 using the same $x_0=[-2,-2]$ and tol=1e-6. Report iteration counts for both methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1942fabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def hess_g(xy: np.ndarray) -> np.ndarray:\n",
    "    # Hessian of g(x,y) = (x-1)^2 + 10*(y-2)^2\n",
    "    # ### BEGIN STUDENT CODE\n",
    "    raise NotImplementedError(\"Implement Hessian of g.\")\n",
    "    # ### END STUDENT CODE\n",
    "\n",
    "def newton_2d(fun, grad_fun, hess_fun, x0, tol=1e-6, max_iter=100):\n",
    "    # Newton's method for R^2 -> R objectives.\n",
    "    # Returns: history array of shape (T, 2).\n",
    "    # ### BEGIN STUDENT CODE\n",
    "    raise NotImplementedError(\"Implement Newton's method.\")\n",
    "    # ### END STUDENT CODE\n",
    "\n",
    "# --- Run & compare ---\n",
    "x0 = np.array([-2.0, -2.0], dtype=float)\n",
    "try:\n",
    "    hist_gd = gradient_descent_2d(g, grad_g, x0, step=0.1, tol=1e-6, max_iter=500)\n",
    "    hist_newton = newton_2d(g, grad_g, hess_g, x0, tol=1e-6, max_iter=100)\n",
    "    print(\"GD iterations:\", len(hist_gd)-1)\n",
    "    print(\"Newton iterations:\", len(hist_newton)-1)\n",
    "except NotImplementedError:\n",
    "    print(\"Finish Q3 and implement Q4 to run the comparison.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5efceac",
   "metadata": {},
   "source": [
    "\n",
    "## Q5. Conditional probability (10 marks)\n",
    "\n",
    "A disease affects 1% of a population. A test for the disease has:\n",
    "- Sensitivity = P(+ | D) = 0.95\n",
    "- Specificity = P(- | not D) = 0.90\n",
    "\n",
    "1. (6 marks) Implement posterior_disease_prob(p_D, sens, spec) that returns P(D | +) via Bayes’ rule.  \n",
    "2. (4 marks) Evaluate with the numbers above and print the result to 4 decimal places. Briefly comment on why the result might be unintuitive.\n",
    "\n",
    "#### Hint:\n",
    "You're asked for the posterior $P(D|+)$ given:\n",
    "- Prior: $P(D)= 0.01$\n",
    "- Sensitivity: $P(+|D)=0.95$ \n",
    "- Specificity: $P(- | \\bar{D}) = 0.90 \\implies$ False-positive Rate $P(+|\\bar{D}) = 1 - 0.90 = 0.10$\n",
    "\n",
    "- Using Bayes' rule, we get the positive predictive value as:\n",
    "$$\n",
    "P(D|+) = \\dfrac{P(+|D)P(D)}{P(+|D)P(D) + P(+|\\bar{D})P(\\bar{D})}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519007ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def posterior_disease_prob(p_D: float, sens: float, spec: float) -> float:\n",
    "    # Return P(D | +) using Bayes' rule.\n",
    "    # p_D: prior P(D)\n",
    "    # sens: P(+ | D)\n",
    "    # spec: P(- | not D)\n",
    "    # ### BEGIN STUDENT CODE\n",
    "    raise NotImplementedError(\"Implement Bayes posterior.\")\n",
    "    # ### END STUDENT CODE\n",
    "\n",
    "# --- Evaluate ---\n",
    "try:\n",
    "    p = posterior_disease_prob(0.01, 0.95, 0.90)\n",
    "    print(f\"P(D | +) = {p:.4f}\")\n",
    "except NotImplementedError:\n",
    "    print(\"Implement posterior_disease_prob to compute the posterior.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36959db",
   "metadata": {},
   "source": [
    "**Your short comment (2–4 lines):**  \n",
    "*Write here why this result might feel counterintuitive (hint: base rate).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eafa64",
   "metadata": {},
   "source": [
    "\n",
    "## Q6. Entropy (12 marks)\n",
    "\n",
    "Let $X$ be a discrete random variable with support $\\{a,b,c,d\\}$ and probabilities\n",
    "$P(a)=0.5, P(b)=0.25, P(c)=0.125, P(d)=0.125.$\n",
    "\n",
    "1. (7 marks) Implement entropy(p) that computes $H(X) = -\\sum_x p(x) * \\log_2 p(x)$.  \n",
    "   Validate that the function ignores zero entries and checks normalisation (within numerical tolerance).\n",
    "2. (5 marks) Compute $H(X)$ for the distribution above and print the value to 3 decimal places.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6458cd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def entropy(p: np.ndarray) -> float:\n",
    "    # Shannon entropy in bits. p is a 1D array of probabilities.\n",
    "    # ### BEGIN STUDENT CODE\n",
    "    raise NotImplementedError(\"Implement entropy.\")\n",
    "    # ### END STUDENT CODE\n",
    "\n",
    "# --- Evaluate ---\n",
    "try:\n",
    "    p = np.array([0.5, 0.25, 0.125, 0.125], dtype=float)\n",
    "    H = entropy(p)\n",
    "    print(f\"H(X) = {H:.3f} bits\")\n",
    "except NotImplementedError:\n",
    "    print(\"Implement entropy to compute H(X).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80d2465",
   "metadata": {},
   "source": [
    "\n",
    "## Q7. Mutual Information I(X;Y) (13 marks)\n",
    "\n",
    "Consider the joint distribution over $X$ in $\\{0,1\\}$ and $Y$ in $\\{0,1\\}$:\n",
    "$ P = \n",
    "\\begin{bmatrix}\n",
    "& 0.30 & 0.10 \\\\\n",
    "& 0.20 & 0.40\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "1. (9 marks) Implement mutual_information(P) that computes\n",
    "$I(X;Y) = \\sum_{x,y} p(x,y) * \\log_2( p(x,y) / (p(x)*p(y)) )$,\n",
    "where $p(x)$ and $p(y)$ are the marginals from P. Handle zeros safely.  \n",
    "2. (4 marks) Compute and print $I(X;Y)$ to 3 decimal places for the matrix above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d871ddc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def mutual_information(P: np.ndarray) -> float:\n",
    "    # Mutual information in bits given a 2D joint table P (rows: X, cols: Y).\n",
    "    # ### BEGIN STUDENT CODE\n",
    "    raise NotImplementedError(\"Implement mutual information.\")\n",
    "    # ### END STUDENT CODE\n",
    "\n",
    "# --- Evaluate ---\n",
    "try:\n",
    "    P = np.array([[0.30, 0.10],\n",
    "                  [0.20, 0.40]], dtype=float)\n",
    "    Ixy = mutual_information(P)\n",
    "    print(f\"I(X;Y) = {Ixy:.3f} bits\")\n",
    "except NotImplementedError:\n",
    "    print(\"Implement mutual_information to compute I(X;Y).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1500eb96",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Academic integrity\n",
    "This is an individual assignment. You may discuss high-level ideas, but all code must be your own. We may run similarity checks.\n",
    "\n",
    "### Good luck!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
