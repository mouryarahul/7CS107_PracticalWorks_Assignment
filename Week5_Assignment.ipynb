{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88c9e264",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mouryarahul/7CS107_PracticalWorks_Assignment/blob/master/Week5_Assignment.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49937e8d",
   "metadata": {},
   "source": [
    "# 7CS107: Advanced AI and Machine Learning — Programming Assignment (Week 5)\n",
    "\n",
    "**Topic coverage:** Decision Theory, Naive Bayes, Logistic Regression, Support Vector Machines (SVM), model evaluation.\n",
    "\n",
    "**Total marks:** 100 (5 questions × 20 marks)\n",
    "\n",
    "**Allowed libraries:** NumPy, SciPy, scikit-learn, pandas, matplotlib/seaborn, and Python standard library.\n",
    "\n",
    "**Submission:** Submit this **executed** notebook (.ipynb) on the Canvas with **all outputs visible**.\n",
    "\n",
    "> **Academic integrity:** Work must be your own. Cite any external sources. You may discuss general ideas, but code must be written independently.\n",
    "\n",
    "---\n",
    "\n",
    "## How to work through this assignment (step-by-step)\n",
    "\n",
    "1. **Read the task** in the markdown cell before each question.\n",
    "2. **Open the code cell** that contains a function with a `# TODO` block.\n",
    "3. **Implement only inside `# TODO`** (do not change function names or signatures).\n",
    "4. **Run the code cell** to define your function.\n",
    "5. **Run the test cell** that follows. The test cell:\n",
    "   - Creates or loads data,\n",
    "   - Calls your function,\n",
    "   - Computes metrics,\n",
    "   - Checks thresholds using `assert` statements.\n",
    "6. If a test fails:\n",
    "   - Read the **inline comments** (every line in the test cells is commented to explain intent),\n",
    "   - Print intermediate values if needed,\n",
    "   - Refine your code and re-run.\n",
    "7. **Keep code concise** (a few lines are sufficient). You may add brief comments to explain your reasoning.\n",
    "8. **Save and re-run `Kernel > Restart & Run All`** before submission to ensure a clean run.\n",
    "\n",
    "### Marking scheme (per question, 20 marks)\n",
    "- **Implementation correctness (12 marks):** passes tests; follows the requested approach.\n",
    "- **Result quality (5 marks):** metrics meet/beat thresholds in the tests (or are well-justified if borderline).\n",
    "- **Code quality (3 marks):** clear, concise, readable; uses appropriate library functions.\n",
    "\n",
    "### Datasets used (auto-downloaded by scikit-learn)\n",
    "- **20 Newsgroups (text)**: `sklearn.datasets.fetch_20newsgroups`\n",
    "- **Breast Cancer Wisconsin**: `sklearn.datasets.load_breast_cancer`\n",
    "- **Iris**: `sklearn.datasets.load_iris`\n",
    "\n",
    "For dataset documentation, see scikit-learn docs (no manual download needed).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a600094a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports used across questions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups, load_breast_cancer, load_iris\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import (confusion_matrix, classification_report, f1_score, accuracy_score,\n",
    "                             roc_auc_score, roc_curve)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Global random seed for reproducibility across tests\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0bd9a0",
   "metadata": {},
   "source": [
    "## Q1 (20 marks) — Decision Theory: Bayes Optimal (Cost-Sensitive) Classifier for Gaussian Models\n",
    "\n",
    "We consider a two-class problem with **known** class-conditional densities and class priors:\n",
    "- $x\\mid y=k \\sim \\mathcal{N}(\\mu_k,\\Sigma_k)$,\n",
    "- Priors $(\\pi_0,\\pi_1)$,\n",
    "- Optional **cost matrix** $C$ where $C[i,j]$ is the cost of predicting class $j$ when the true class is $i$.\n",
    "\n",
    "### Your task\n",
    "1. Compute the **posterior** $P(y=k\\mid x)$ using Bayes' rule. Use **log-densities** for numerical stability.\n",
    "2. If a cost matrix is supplied, choose the class that **minimizes expected risk**:\n",
    "   $$\\hat{y}(x)=\\arg\\min_j \\sum_i C[i,j] \\cdot P(y=i\\mid x).$$\n",
    "   If `cost_matrix=None`, perform **MAP** (argmax posterior).\n",
    "\n",
    "### Implementation tips\n",
    "- Use `scipy.stats.multivariate_normal.logpdf` for $\\log p(x\\mid y=k)$.\n",
    "- Convert log-posteriors to probabilities with a log-sum-exp trick: subtract the row-wise max, exponentiate, and normalize.\n",
    "- Return a 1D array of predicted class indices of shape `(n,)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308c0a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, Optional\n",
    "\n",
    "def bayes_decision_gaussian(X: np.ndarray,\n",
    "                            priors: Sequence[float],\n",
    "                            mus: Sequence[np.ndarray],\n",
    "                            covs: Sequence[np.ndarray],\n",
    "                            cost_matrix: Optional[np.ndarray] = None) -> np.ndarray:\n",
    "    \"\"\"Bayes (cost-sensitive) classifier for Gaussian class-conditionals.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : (n, d) array of inputs.\n",
    "    priors : length-K sequence of class priors that sum to 1.\n",
    "    mus : list of K arrays of shape (d,) (class means).\n",
    "    covs : list of K arrays of shape (d,d) (class covariances).\n",
    "    cost_matrix : (K,K) array or None. If None, do MAP; else minimize expected risk.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y_pred : (n,) array of predicted class indices (0..K-1)\n",
    "    \"\"\"\n",
    "    X = np.asarray(X)\n",
    "    K = len(priors)\n",
    "    n = X.shape[0]\n",
    "\n",
    "    # TODO: Compute unnormalized log-posteriors: log p(x|y=k) + log pi_k\n",
    "    log_posts = np.zeros((n, K))\n",
    "        \n",
    "    # TODO: Normalize to get posteriors using log-sum-exp stabilization\n",
    "    # subtract row-wise max\n",
    "    # exponentiate stabilized values\n",
    "    # row-wise normalization to 1\n",
    "\n",
    "    if cost_matrix is None:\n",
    "        # return MAP decision\n",
    "\n",
    "    # TODO: Expected risk for predicting class j: sum_i C[i,j] * P(y=i|x)\n",
    "    # compute expected risks for all classes\n",
    "    # return minimize expected risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd28d66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion (cost-sensitive): [[74 16]\n",
      " [ 1 29]]\n",
      "Confusion (MAP): [[86  4]\n",
      " [ 5 25]]\n",
      "Empirical expected risk (cost-sensitive): 0.175\n",
      "Empirical expected risk (MAP):           0.242\n",
      "[Q1] Tests passed ✅\n"
     ]
    }
   ],
   "source": [
    "# === Tests for Q1 ===\n",
    "np.random.seed(0)  # set seed for this test to make it reproducible\n",
    "\n",
    "# --- Create a synthetic 2D Gaussian dataset for two classes ---\n",
    "mu0 = np.array([0.0, 0.0])                # mean of class 0\n",
    "mu1 = np.array([2.0, 2.0])                # mean of class 1\n",
    "Sigma0 = np.array([[1.0, 0.2],            # covariance of class 0\n",
    "                   [0.2, 1.0]])\n",
    "Sigma1 = np.array([[1.0, -0.3],           # covariance of class 1\n",
    "                   [-0.3, 1.2]])\n",
    "priors = [0.6, 0.4]                       # class priors p(y=0)=0.6, p(y=1)=0.4\n",
    "\n",
    "# Sample points from each class\n",
    "n0, n1 = 90, 30                          # number of samples per class\n",
    "X0 = np.random.multivariate_normal(mu0, Sigma0, size=n0)  # samples of class 0\n",
    "X1 = np.random.multivariate_normal(mu1, Sigma1, size=n1)  # samples of class 1\n",
    "X = np.vstack([X0, X1])                    # stack into a single dataset (n, d)\n",
    "y_true = np.hstack([np.zeros(n0, dtype=int), np.ones(n1, dtype=int)])  # true labels (n,)\n",
    "\n",
    "# --- Define a cost matrix to penalize false negatives more heavily ---\n",
    "# cost[i,j] = cost of predicting j when true class is i\n",
    "cost = np.array([[0.0, 1.0],               # predicting 1 when true is 0 costs 1\n",
    "                 [5.0, 0.0]])              # predicting 0 when true is 1 costs 5 (more serious)\n",
    "\n",
    "# --- Call the student's function in two modes: cost-sensitive and MAP ---\n",
    "y_pred_cost = bayes_decision_gaussian(X, priors, [mu0, mu1], [Sigma0, Sigma1], cost_matrix=cost)  # minimize expected risk\n",
    "y_pred_map  = bayes_decision_gaussian(X, priors, [mu0, mu1], [Sigma0, Sigma1], cost_matrix=None)  # MAP (no costs)\n",
    "\n",
    "# --- Compute confusion matrices to see error patterns ---\n",
    "conf_cost = confusion_matrix(y_true, y_pred_cost, labels=[0, 1])  # confusion for cost-sensitive predictions\n",
    "conf_map  = confusion_matrix(y_true, y_pred_map,  labels=[0, 1])  # confusion for MAP predictions\n",
    "\n",
    "# --- Compute empirical expected risk for each strategy ---\n",
    "# Multiply elementwise by the cost matrix and average by number of samples\n",
    "risk_cost = (conf_cost * cost).sum() / len(X)\n",
    "risk_map  = (conf_map  * cost).sum() / len(X)\n",
    "\n",
    "# --- Display diagnostics ---\n",
    "print(\"Confusion (cost-sensitive):\", conf_cost)\n",
    "print(\"Confusion (MAP):\", conf_map)\n",
    "print(f\"Empirical expected risk (cost-sensitive): {risk_cost:.3f}\")\n",
    "print(f\"Empirical expected risk (MAP):           {risk_map:.3f}\")\n",
    "\n",
    "# --- Assertion: cost-sensitive decision should not have higher expected risk than MAP here ---\n",
    "assert risk_cost <= risk_map + 1e-6, \"Cost-sensitive decision should not yield higher expected risk than MAP here.\"\n",
    "print(\"[Q1] Tests passed ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25092cc8",
   "metadata": {},
   "source": [
    "## Q2 (20 marks) — Naive Bayes on Text (20 Newsgroups)\n",
    "\n",
    "Train a **Multinomial Naive Bayes** classifier on a **2-class** subset of the 20 Newsgroups dataset (default: `sci.space` vs `rec.autos`).\n",
    "\n",
    "### Your task\n",
    "1. Load train **and** test splits for the chosen categories with `fetch_20newsgroups` (remove headers/footers/quotes).\n",
    "2. Vectorize text using `CountVectorizer(min_df=2)` (bag-of-words counts).\n",
    "3. Fit `MultinomialNB(alpha=alpha)` on training data and predict on test data.\n",
    "4. Return **macro F1** on the test set.\n",
    "\n",
    "### Tips\n",
    "- Keep code compact (4–6 lines inside the function is enough).\n",
    "- Use provided defaults: `categories=(\"sci.space\",\"rec.autos\")`, `alpha=1.0`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab1e73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nb_20ng(categories=(\"sci.space\", \"rec.autos\"), alpha: float = 1.0) -> float:\n",
    "    \"\"\"Train MultinomialNB on a two-class 20NG subset and return macro F1 on the test set.\"\"\"\n",
    "    # TODO: load train & test data for the specified categories\n",
    "    data_train = fetch_20newsgroups(subset='train', categories=list(categories), remove=('headers', 'footers', 'quotes'))\n",
    "    data_test  = fetch_20newsgroups(subset='test',  categories=list(categories), remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "    # TODO: vectorize text and fit NB\n",
    "    # Use CountVectorizer with min_df=2 to vectorize the text data\n",
    "    # Fit and transform the training data\n",
    "    # Transform the test data\n",
    "\n",
    "    # create Multinomial Naive Bayes classifier with given alpha\n",
    "    # fit the model\n",
    "    # predict on test data\n",
    "\n",
    "    # TODO: compute macro F1\n",
    "    # return macro F1 score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd7fb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1: 0.898\n",
      "[Q2] Tests passed ✅\n"
     ]
    }
   ],
   "source": [
    "# === Tests for Q2 ===\n",
    "# Train on the default categories and evaluate macro F1\n",
    "f1 = train_nb_20ng()                                     # run train_nb_20ng function with defaults\n",
    "print(f\"Macro F1: {f1:.3f}\")                             # display the macro-averaged F1 score\n",
    "\n",
    "# Minimal performance threshold for this simple baseline\n",
    "assert f1 >= 0.75, \"F1 should be at least 0.75 on this binary subset with bag-of-words + NB.\"\n",
    "print(\"[Q2] Tests passed ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e060c13d",
   "metadata": {},
   "source": [
    "## Q3 (20 marks) — Logistic Regression (Breast Cancer) with ROC–AUC\n",
    "\n",
    "Train a **Logistic Regression** classifier on the Breast Cancer Wisconsin dataset. Use a train/test split (stratified), a standardization step, and report **ROC–AUC** on the test set.\n",
    "\n",
    "### Your task\n",
    "1. Split into train/test with `train_test_split(..., stratify=y, test_size=0.25, random_state=42)`.\n",
    "2. Build a pipeline: `StandardScaler()` → `LogisticRegression(max_iter=1000, solver='liblinear')`.\n",
    "3. Return **ROC–AUC** on the test set using predicted probabilities.\n",
    "\n",
    "### Tips\n",
    "- The `solver='liblinear'` works well for smaller datasets; keep defaults unless you experiment.\n",
    "- Expose `C` and `class_weight` as parameters to the function and pass them into the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fa2154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_breast_cancer(C: float = 1.0, class_weight=None) -> float:\n",
    "    \"\"\"Train LogisticRegression on breast cancer dataset and return ROC–AUC on test set.\"\"\"\n",
    "    # TODO: load data and split\n",
    "    # load breast cancer dataset\n",
    "    # split into train and test sets\n",
    "\n",
    "    # TODO: pipeline and fit\n",
    "    # create pipeline with scaling and logistic regression\n",
    "    # fit the model\n",
    "\n",
    "    # TODO: ROC–AUC on test set\n",
    "    # probability of positive class\n",
    "    #  return ROC-AUC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9fb42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Tests for Q3 ===\n",
    "auc = logreg_breast_cancer()                          # call logreg_breast_cancer with defaults\n",
    "print(f\"Test ROC–AUC: {auc:.3f}\")                     # display the ROC–AUC on the test split\n",
    "\n",
    "# Require a strong baseline with proper scaling (typical performance is high on this dataset)\n",
    "assert auc >= 0.95, \"ROC–AUC should be at least 0.95 on Breast Cancer with scaling + LR.\"\n",
    "print(\"[Q3] Tests passed ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0492b42e",
   "metadata": {},
   "source": [
    "## Q4 (20 marks) — RBF SVM (Iris) with Cross-Validation\n",
    "\n",
    "Train an **RBF-kernel SVM** on the Iris dataset using a pipeline (scaling + SVC). Report **mean accuracy** using stratified 5-fold cross-validation.\n",
    "\n",
    "### Your task\n",
    "1. Build pipeline: `StandardScaler()` → `SVC(kernel='rbf', C=C, gamma=gamma)`.\n",
    "2. Compute **mean accuracy** via stratified K-fold CV (default `cv=5`, shuffled with `random_state=42`).\n",
    "3. Return the **mean** CV accuracy.\n",
    "\n",
    "### Tips\n",
    "- Use `cross_val_score` with `StratifiedKFold` to ensure class balance across folds.\n",
    "- Keep defaults unless you want to experiment with `C` and `gamma`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39176a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_rbf_iris(C: float = 1.0, gamma='scale', cv: int = 5) -> float:\n",
    "    \"\"\"Train SVC with RBF kernel on Iris and return mean CV accuracy.\"\"\"\n",
    "    # TODO: load data and split\n",
    "    # create pipeline with scaling and SVC\n",
    "    # perform stratified CV\n",
    "    # return mean accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd07833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Tests for Q4 (fully commented) ===\n",
    "acc = svm_rbf_iris()                              # run svm_rbf_iris with defaults\n",
    "print(f\"Mean CV accuracy: {acc:.3f}\")             # display mean accuracy across folds\n",
    "\n",
    "# The Iris dataset is clean and low-dimensional; RBF SVM typically achieves high accuracy\n",
    "assert acc >= 0.95, \"Mean CV accuracy should be at least 0.95 on Iris with RBF SVM.\"\n",
    "print(\"[Q4] Tests passed ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e271e8",
   "metadata": {},
   "source": [
    "## Q5 (20 marks) — Nested Cross-Validation for SVM (Breast Cancer)\n",
    "\n",
    "Perform **nested cross-validation** to estimate the generalization performance of an RBF SVM on the Breast Cancer dataset while tuning hyperparameters $(C, \\gamma)$.\n",
    "\n",
    "### Your task\n",
    "1. **Outer loop**: `StratifiedKFold(outer_k, shuffle=True, random_state=42)` splits data into train/test.\n",
    "2. **Inner loop**: `GridSearchCV` over a parameter grid for `C` and `gamma` with `StratifiedKFold(inner_k, shuffle=True, random_state=123)`.\n",
    "3. For each outer split, fit the inner **grid search** on the outer-train portion, then evaluate the **best model** on the outer-test portion.\n",
    "4. Return `(mean_acc, std_acc)` across all outer folds.\n",
    "\n",
    "### Tips\n",
    "- Build pipeline: `StandardScaler()` → `SVC(kernel='rbf')` and name parameters in grid as `svc__C`, `svc__gamma`.\n",
    "- Keep grids small for runtime: defaults `(0.1, 1, 10)` for `C`, and `('scale', 0.01, 0.1, 1.0)` for `gamma`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6799c3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_cv_svm_breast(outer_k: int = 3, inner_k: int = 3,\n",
    "                         Cs = (0.1, 1, 10), gammas = ('scale', 0.01, 0.1, 1.0)):\n",
    "    \"Perform nested CV with SVM on breast cancer and return mean and std of outer accuracies.\"\"\"\n",
    "    # TODO: load data\n",
    "    ds = load_breast_cancer()\n",
    "    X, y = ds.data, ds.target\n",
    "\n",
    "    #TODO: perform outer CV (stratified)\n",
    "    outer_cv = # your code goes here\n",
    "    outer_scores = []\n",
    "    \n",
    "    # TODO: for each outer fold, perform inner CV grid search and evaluate on outer test fold\n",
    "    for train_idx, test_idx in outer_cv.split(X, y):\n",
    "        # Split data for this outer fold\n",
    "        X_tr, X_te = X[train_idx], X[test_idx]\n",
    "        y_tr, y_te = y[train_idx], y[test_idx]\n",
    "\n",
    "        # Perform Inner CV grid search on the training portion of the outer fold\n",
    "       \n",
    "        # Evaluate the best model found in inner loop on the held-out outer test split\n",
    "\n",
    "    # return mean and std of outer accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbfdc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Tests for Q5 ===\n",
    "# Run nested CV with default outer/inner folds and parameter grids\n",
    "mean_acc, std_acc = nested_cv_svm_breast()                      # returns mean and std accuracy across outer folds\n",
    "print(f\"Outer mean accuracy: {mean_acc:.3f} ± {std_acc:.3f}\")   # display aggregate performance\n",
    "\n",
    "# Expect strong performance on Breast Cancer with proper scaling and model selection\n",
    "assert mean_acc >= 0.93, \"Outer mean accuracy should be at least 0.93 for SVM on Breast Cancer.\"\n",
    "print(\"[Q5] Tests passed ✅\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
